########################################
06-16-20
########################################

To-Do:
-Look into plotallinputmaps in simdat.py
	:Gives connectivity plot of pre/post synaptic neurons
	-Make similar function to plot recurrent connectivity maps with Haroon


########################################
06-15-20
########################################
Working on X2Go vpn connection

Need to clone repository (https://github.com/NathanKlineInstitute/SMARTAgent.git) onto workspace
Need to learn GitHub terminal commands to understand how to do this

Motion of paddle seems to be after an average three null inputs after a motion input. Opposite motion input reverses direction faster. 

If motion is total 100 pixels, it seems 25~30 pixels after input motions, then 45~50, 15, 10 pixels after null inputs.

Sam wants to talk about motion with me and Haroon.
	Is there a way for system to measure what each input does, through experimenting itself?
	Will help transition network to other games.


To-Do:
-Get X2GO to run simulation
	*DONE
-Learn Github
	*DONE. Here is cheatsheet: https://gist.github.com/cferdinandi/ef665330286fd5d7127d
- Talk to Haroon about keeping paddle in middle
	-Count up/down paddle actions
	-Count ball actions to cross board
		*Working on excel sheet to count these
	**DONE. Ball_action_avg=36, Padd_action_avg=10
	-Work on paddle momentum 
		*Github post was responded to
		**It seems every action (up/down) takes multiple frames to be completed. I did up/down action (2/5) followed by no action (0) to see how many frames each input takes, and it seems to be three additional inputs. So it takes four 0 inputs to stop motion.
		**Completed on excel.
-Look into plotallinputmaps in simdat.py
	:Gives connectivity plot of pre/post synaptic neurons
	-Make similar function to plot recurrent connectivity maps with Haroon


########################################
06-12-20
########################################

Paddle seems to have momentum in all pong versions
Frameskip seems to make it harder to control paddle, as if we lose opportunity to move the paddle by skipping frames

Opened issue on GitHub to ask momentum question: 
https://github.com/openai/gym/issues/1954

If all else fails, I can make our own environment for pong:
https://github.com/openai/gym/blob/master/docs/creating-environments.md

On vpn "ssh davidd@10.76.253.155"

########################################
##Different versions of pong
########################################
https://github.com/openai/gym/blob/master/gym/envs/__init__.py

EnvSpec(Pong-v0), EnvSpec(Pong-v4) 
EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4)
EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4)
EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4)
EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4)
EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4)

register(
            id='{}-v0'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type, 'repeat_action_probability': 0.25},
            max_episode_steps=10000,
            nondeterministic=nondeterministic,
        )
        register(
            id='{}-v4'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type},
            max_episode_steps=100000,
            nondeterministic=nondeterministic,
        )
        # Standard Deterministic (as in the original DeepMind paper)
        if game == 'space_invaders':
            frameskip = 3
        else:
            frameskip = 4
        # Use a deterministic frame skip.
        register(
            id='{}Deterministic-v0'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': frameskip, 'repeat_action_probability': 0.25},
            max_episode_steps=100000,
            nondeterministic=nondeterministic,
        )
        register(
            id='{}Deterministic-v4'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': frameskip},
            max_episode_steps=100000,
            nondeterministic=nondeterministic,
        )
        register(
            id='{}NoFrameskip-v0'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': 1, 'repeat_action_probability': 0.25}, # A frameskip of 1 means we get every frame
            max_episode_steps=frameskip * 100000,
            nondeterministic=nondeterministic,
        )
        # No frameskip. (Atari has no entropy source, so these are
        # deterministic environments.)
        register(
            id='{}NoFrameskip-v4'.format(name),
            entry_point='gym.envs.atari:AtariEnv',
            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': 1}, # A frameskip of 1 means we get every frame
            max_episode_steps=frameskip * 100000,
            nondeterministic=nondeterministic,

########################################
##Running controlled pong
########################################
https://github.com/openai/gym/blob/7006c7a182a991b9409c206c56abc121eaffafb5/gym/envs/atari/atari_env.py


import gym; env = gym.make('PongNoFrameskip-v4’); env.reset()
##CHANGE caction to number 1, 3, 4 to move paddle, and repeat this command
observation, reward, done, info = env.step(caction); env.render()


ACTION_MEANING = {
    0: "NOOP",
    1: "FIRE",
    2: "UP",
    3: "RIGHT",
    4: "LEFT",
    5: "DOWN",
    6: "UPRIGHT",
    7: "UPLEFT",
    8: "DOWNRIGHT",
    9: "DOWNLEFT",
    10: "UPFIRE",
    11: "RIGHTFIRE",
    12: "LEFTFIRE",
    13: "DOWNFIRE",
    14: "UPRIGHTFIRE",
    15: "UPLEFTFIRE",
    16: "DOWNRIGHTFIRE",
    17: "DOWNLEFTFIRE",
}

########################################
##PRIOR NOTES
########################################

TO RUN SIM (at 2 cores)
./myrun 2 sim.jsons
mpiexec -n 2 nrniv -python -mpi sim.py

simdat.py is used to load output data from a simulation you ran. so to create that final weight output file you'd first run the simulation (which saves the data), then python -i simdat.py backupcfg/yoursimfilename.json to load the data. then this command pdfs = pdf[pdf.time==np.amax(pdf.time)]; D = pdf2weightsdict(pdfs); pickle.dump(D, open('data/'+simstr+'synWeights_final.pkl','wb'))  will save the final weights to a new output file. then you'd specify that new file in sim.json ... some of that could be automated

and when you specify that new file, it will be for the ResumeSimFromFile field

yeah, or can use multistepsim.py
which does most of that for you
python multistepSim.py sim.json 12 10 multirun
so that will run sim.json with 12 cores 10X loading weights from last run into next run sequentially
i don't always use it so i'd have a chance to look at output before starting next one...