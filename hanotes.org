*19nov7
**I ran a simulation with overlapped field of view i.e. every neuron in layer V1 received input from 25 neurons in a topological manner.
**I tested a couple of initial weights to see how the synaptic weights evolve
***The results are very similar (by eye balling) when initial weight is 0.001 vs 0.0012
****I feel that the topological mapping from layer R to layer V1 would not be a useful strategy at this time because i think that having biological realism in model will require completeness too to make the model useful. And with just a couple of layers to encode image sequences, having topological mapping limits the capacity of the network to encode the changes in virtual game environment and later associate actions to the sequence of images.
samn 10:23 AM
why does topology limit capacity of network to encode changes and associate actions?
10:24 AM
can objects be represented without topology?
Haroon Anwar 10:24 AM
because a couple of layers would not allow merging localized information into spatial sequences
10:24 AM
“can objects be represented without topology?” - no
samn 10:25 AM
how many layers are needed to merge local info into spatial sequence?
Haroon Anwar 10:26 AM
not sure…. would guess 3-4 or so
samn 10:26 AM
can try that then
10:26 AM
and have a variable control whether to use topological mapping
Haroon Anwar 10:27 AM
i am not against topological mapping….just trying to think the potential issues as we build up network
samn 10:27 AM
it would be interesting if the system worked without topological mapping, but also surprising
10:27 AM
since you would think it needs some representation of space including positioning of objects
Haroon Anwar 10:28 AM
i like to make things as biological as possible but then it also require completeness and we dont know how much detail would be enough to capture something at biological level
samn 10:28 AM
if pixels from two rackets are activated with same neurons, how is the neuronal representation separated?
10:28 AM
i'm not sure if we need completeness for what we are doing. besides that, we will never have completeness
Haroon Anwar 10:28 AM
thats an interesting point
10:28 AM
completeness for a task
samn 10:28 AM
ic
Haroon Anwar 10:29 AM
like we talked about layers
samn 10:29 AM
right, we don't know how much detail needed
10:29 AM
can only guess/hypothesize until we have a way to test the system
Haroon Anwar 10:30 AM
when things are merged into a single or two layers and everything is connected to everything, the network can be trained to do a task without making sense
10:30 AM
just increase the number of neurons and connections in the middle layer and it does the magic
samn 10:30 AM
that would be fine too
10:30 AM
even that with more realistic neurons is a step towards more realism
10:30 AM
but my sense is that topology is important for reason mentioned above
Haroon Anwar 10:31 AM
“but my sense is that topology is important for reason mentioned above” - cant agree more…
samn 10:31 AM
you should try both ways
**** 1 Possible solution: "The role of competitive inhibition and top-down feedback in binding during object recognition"
*****In this regard, use Bhezhenov strategy.
****Another issue in the way the model is setup for learning is: hardly a couple of successful events occur (i.e. the player hits the ball with the racket). I wonder what will be a better strategy to overcome this problem.
10:07 AM
not immediate problem but it will be a problem
samn 10:07 AM
can you clarify the problem?
Haroon Anwar 10:07 AM
and we need to find a solution for this
10:07 AM
ok
10:07 AM
in game there are 2 players
10:08 AM
game allows access to 1 player
10:08 AM
its actions and reward
10:08 AM
within each episode, if the player plays randomly, it hardly plays a correct move and gets rewarded
10:14 AM
the player using random strategy gets rewarded, but less than a player with a good strategy
Haroon Anwar 10:14 AM
yes
samn 10:14 AM
and the random strategy also produces punishment too
Haroon Anwar 10:15 AM
i am not sure about that….
10:15 AM
i just noticed reward to be 0 or 1
samn 10:16 AM
well, we will have to build in some punishment too
10:16 AM
for all games they only use 0 or 1 ?
Haroon Anwar 10:16 AM
not sure
10:16 AM
need to check that
samn 10:16 AM
we could make rules such as if no reward for a while, produce a punishment
Haroon Anwar 10:16 AM
i see
samn 10:17 AM
but hopefully they have explicit punishments already. yeah, good to think about different approaches for those issues
Haroon Anwar 10:17 AM
OK
10:17 AM
but i was thinking, how we learn is not only by getting results of our actions
10:18 AM
but also looking at others, how they play
samn 10:18 AM
true
Haroon Anwar 10:18 AM
and try to learn some rules
samn 10:18 AM
there's something called teacher-based learning
10:18 AM
imitation, etc.
Haroon Anwar 10:18 AM
ok
samn 10:19 AM
can try some of that as well...e.g. produce a population of models, transfer some weights from strong models to weaker models
10:19 AM
in that case it's similar to evolution
Haroon Anwar 10:19 AM
yes. so all of these are possible strategies. that can be pursued in parallel
samn 10:19 AM
for more complicated teacher learning have to develop an algorithm that can observe and copy
Haroon Anwar 10:20 AM
agree… but will also need access to rewards and association to actions of other player
samn 10:20 AM
yeah
****Can agent learn about good actions from the computer (i.e. the other player)? A couple of problems in that regard are lack of access to the information regarding the other player's actions and reward.
**To Do List: 1. Include inhibition in the model. 2. Implement a strategy for STDP where the strength of synapses slowly decay over time unless it hits a threshold, in which case, it stays strengthened.
*19nov8
**To Do List (side tracked): 1. Include inhibition in the model. 2. Before implementing homeostatic STDP, implement Bhezhenov model. This model should exist in parallel. 3. Implement simplified motor cortex and use reward based STDP to train it. 4. Implement homeostatic STDP
***PROBLEM UNDERSTANDING BAZHENOV ARCHITECTURE- disuccion with same
****Haroon Anwar 9:53 AM
it says every neuron send an excitatory and inhibitory synapse to 9 neurons
samn 9:53 AM
that's strange
samn 9:53 AM
we of course don't have to follow his paper exactly
Haroon Anwar 9:53 AM
read figure 1 legend
samn 9:53 AM
it's just a general framework, which we've developed similarly previously too
Haroon Anwar 9:54 AM
i understand that. but just trying to understand what they did
samn 9:54 AM
they tried to balance the output by having feedforward inhibition?
Haroon Anwar 9:54 AM
i thought may be i dont understand English
samn 9:54 AM
ic
9:54 AM
you probably understand, but their rule does seem unusual
Haroon Anwar 9:55 AM
so same neuron excites postsynaptic neuron and inhibits too
9:55 AM
but then there has to be some rule to have a balanced excitation
9:55 AM
so its not real inhibition as in biological network
samn 9:56 AM
they might have different delays and time constants, as well as how far it spreads spatially
Haroon Anwar 9:56 AM
its balancing inhibition
samn 9:56 AM
let me read more of the details
Haroon Anwar 9:56 AM
please
samn 10:00 AM
"Output balancing.
Our previous study revealed the importance of balancing the strength of the output synapses [30]. In this new study, implementation of the output part of synaptic balancing was to reduce the rate of synaptic growth in the neurons that already had high total synaptic output. This effectively prevented a very small number of neurons from controlling the entire network. Thus, for each middle-layer cell, increments of the strength of outgoing synapses resulting from rewarded STDP events were divided by the ratio of the current sum of synaptic outputs to the initial sum of synaptic outputs of the same cells (see Methods). The result was that synapses originating from the neurons with many strong outputs were not able to increase their synaptic strength as quickly as synapses from the neurons with a weak output. This gave a competitive advantage to the later neurons. It helped to control synaptic output, thus preventing over-representation by the cells whose activities were most often correlated with the rewards (see S1 Fig). The performance of the full model simulated without this rule is shown by the red line in Fig 3."
10:00 AM
so they increment the input synapse of cells that already have high weights by a smaller increment?
10:00 AM
sorry, the output
10:00 AM
synapses
Haroon Anwar 10:01 AM
but how does inhibition play role in that
samn 10:01 AM
yeah, it doesn't mention inhibition there
10:01 AM
where was the inhibition mentioned?
Haroon Anwar 10:01 AM
in the architecture
samn 10:02 AM
ic, Feedforward synaptic inhibition was implemented in the model (see Methods) and was necessary for optimal behavior of the network. Thus each layer projecting excitatory connections to the following layer was also projecting inhibition and the total strength of inhibition was equal to the total strength of excitation.
Haroon Anwar 10:02 AM
yes that too
10:02 AM
dont understand the role of this inhibition
samn 10:03 AM
some way to maintain sparse firing and prevent incorrect output
Haroon Anwar 10:03 AM
i see
samn 10:04 AM
they don't seem to explain too much at finer level of detail
Haroon Anwar 10:04 AM
no
samn 10:04 AM
we could test that later on. see how individual synapses being turned on or off impact very specific behaviors
Haroon Anwar 10:04 AM
ok
samn 10:04 AM
or groups of synapses
10:05 AM
did you see the mod file i had to maintaining a target firing rate? it's just one way to have homeostatic weights
10:05 AM
i mean homeostatic synaptic scaling
10:05 AM
/u/samn/syscale
Haroon Anwar 10:05 AM
so for now, previously i was thinking about having inhibition driven by inhibitory neurons
10:05 AM
no
10:05 AM
can you send me that
samn 10:06 AM
yeah, i agree we should keep inhibitory and excitatory neurons separate

 *** Do we need inhibition to be cell based (inhibition implements contrast enhancement or define receptive fields) or region based (with some temporal difference different hierarchical regions inhibit other regions)?
 ***for connectivity could have broad inputs to interneurons, with some spatial dependence on prbability too
 ***factors for connectivity: how strongly the E cells are activated will determine how strongly the I cells have to get activated
spatial dependence of E -> E wiring will influence spatial dependence of E -> I and I -> I and I -> E wiring
for now i would make it parameterized so you can play with it and see what works
***O-Reilly model uses k-winner-take-all competitive inhibition:
****Input; V1 - 3600 neurons; V2/V4 (overlapping field of views) - 2800 neurons that receive from 320 neighboring V1 neurons; IT - 200 neurons - receive inputs from 2800 neurons; Feedforward - 80-90%; Feedback between adjacent - 10-20%; k-winner take all inhibitory competition rule — k most active units remain active over time. each layer has different k, but is generally in the range of 10-20% of neurons in the layer.
***for network balacing use sam's homeostatic synaptic scaling

*19nov12
**Things to do today: 
***1. to implement inhibition, use gabaa syn model. 
***2. drive inhibitory neurons using hh model but with higher firing rate then excitatory neurons.
***3. Extend Bazhenov architecture to O'Reilly architecture (Front. Psych. 2012 paper):
****Input; V1 - 3600 neurons; V2/V4 (overlapping field of views) - 2800 neurons that receive from 320 neighboring V1 neurons; IT - 200 neurons - receive inputs from 2800 neurons; Feedforward - 80-90%; Feedback between adjacent - 10-20%; k-winner take all inhibitory competition rule — k most active units remain active over time. each layer has different k, but is generally in the range of 10-20% of neurons in the layer.
****Base on O'Reilly's architecture: try using R: 6400, V1: 6400, each neuron in V1 receive input from overlapping 25 neurons in R. 1600 neurons in V2/V4. each neuron in V2/V4 receives 25 neurons from V1. 400 neurons in IT with each neuron receiving inputs from 25 neurons in V2/V4. 

*19nov13
*** Based on the paper "Recruitment of inhibition and excitation across mouse visual cortex depends on the hierarchy of interconnecting areas by Rinaldo David D’Souza1*, Andrew Max Meier1, Pawan Bista1, Quanxin Wang2, Andreas Burkhalter1"
****In L2/3 FF (V1->PM) pathway, EPSC recorded from PV cells were larger than those from Pyr cells.
****FF excitation of inhibitory PV neurons relative to neighboring Pyr neuron is stronger than FB excitation of PV neurons. In both cases PV excitation is stronger than Pyr excitation.
****Larger EPSC in PV cells could be a result of either higher density of excitatory input (higher weight of synaptic connections) or due to larger area over which individual PV cells are contacted by inter areal projections, or both.
****The difference in relative excitation of PV and Pyr was bigger in FF(V1->PM) than in FF(LM->PM: the pathway is V1->LM->PM). 
****in contrast, the difference was smaller in FB (PM->V1) than FB(LM->V1).
****Rules are different in L5.

*19nov14
**Implemented divergent connectivity function
**Included inhibitory neurons: 1600 InV1, 400 InV4 and 100 InIT. These neurons are driven by excitation from R, V1 and V4 respectively.
***for all pre(E) to post(I), the overlap is different: 15x15 for R to InV1, 25x25 for V1 to InV4 and 25x25 for V4 to InIT.
**Feedback inhibition implemented: InV1 inhibit neurons in R, InV4 inhibit neurons in V1 and InIT inhibit neurons in V4.
***for all pre(I) to post(E), the overlap is 5x5 neurons i.e. every post synaptic neuron receives inhibition from 25 neighboring neurons. 
**To DO: add poisson noise to inhibitory neurons so that the neurons are close to threshold. this is to increase firing rate of inhibitory neurons.

*19nov15
**Run the simulation for 10s with same firing rate for both excitatory and inhibitory neurons. Only save the raster.
**Add noise stim to inhibitory neurons to increase their firing rates and run the simulation.
**I had only feedback inhibition and feedforward excitation. Now i have included feedforward inhibition as well as feedbackward excitation.
***QUESTION: Is there any evidence of feedforward inhibition and feedbackward excitation in cortex? any biological numbers for such connectivity?
**To do: implement local inhibition- previously we had only feedforward and feedbackward inhibition.
***What are the rules for local inhibition in terms of receptive fields.

*19nov18
**Ran simulation for 10 sec with strong feedback excitation (weight of 0.02)-- observed high firing rates for excitatory and inhibtory neurons (just obseration)
**Ran simulation for 10 sec without feedback excitation-- observed firing rates (R = 0.686, V1 = 1.96, V4 = 3.75, IT = 11.7, IV1 = 21.2, IV4 = 11, IIT = 15.2 Hz)
**Ran simulation for 10 sec with weaker feedback excitation (weight of 0.002)-- observed firing rates (R = 0.7, V1 = 2.5, V4 = 4.85, IT = 11.9, IV1 = 21.2, IV4 = 11.2, IIT = 14.2 Hz)
**Running simulation for 20 sec with weaker feedback excitation (weight of 0.002) and saving at 2 ms instead of 0.2 ms
**Include local inhibition and excitation.

*19nov19
**Include local inhibition and excitation (using some general statistics).

**19nov20
**Tried installing atari environment for gym open ai on neurosim, but due to dependencies could not succeeded. 
**Tried installing atari environment for gym open ai on KONG, which worked but netpyne didn't work becaue NEURON is not installed with python3.
***pip install gym --user
***pip install atari-py --user
***Contacted ARCS for help in installing NEURON with python3
** Tried again installing atari environment for gym open ai on neurosim: tried on no.neurosim.downstate.edu
***pip3 install gym --user (it worked)
***pip3 install atari-py --user (it worked)
**Running trainSmartAgent.py on neurosim. (it could not run render therefore i commented out render command)
**Compare neurosim vs personal magic


***NEUROSIM
Creating network of 7 cell populations on 1 hosts...
  Number of cells on node 0: 16900 
  Done; cell creation time = 2.68 s.
Making connections...
  Number of connections on node 0: 2318471 
  Done; cell connection time = 430.00 s.
Adding stims...
  Number of stims on node 0: 8500 
  Done; cell stims creation time = 1.27 s.


***Personal Mac
Creating network of 7 cell populations on 1 hosts...
  Number of cells on node 0: 16900 
  Done; cell creation time = 1.61 s.
Making connections...
  Number of connections on node 0: 2318471 
  Done; cell connection time = 325.40 s.
Adding stims...
  Number of stims on node 0: 8500 
  Done; cell stims creation time = 0.82 s.

*19nov21
***To run simulation on neurosim- always use py3env to use python3.
***To run simulation in parallel using 8 cores on neurosim - use mpirun -n 8 python trainSmartAgent.py
***run time on neurosim using 8 cores - Done; run time = 5936.18 s; real-time ratio: 0.00.
***One problem remains: Plotting raster plot.
***Additional analyses to include: evolving spike rates and .....

*19nov26
**Yesterday, I made changes in the script to save the firing times of all cells.
**Salva told me that we can only use .pkl or .json to save data from plotRaster
***e.g. simConfig.analysis['plotRaster'] = {'popRates':'overlay','saveData':'RasterData.pkl','showFig':True}

*19dec4
**Yesterday, I ran analyzeSpikeRate.py on neurosim to analyze population average firing rates. I ntoiced extremely high values for frequencies, which was due to a bug in the code.
**Today, i fixed the code and reran on neurosim. Firing rate of all populations rose initially and then decayed to a stable firing rate. This analysis was done on 100 s of simulation.
**Nest step is to implement motor cortex. First, i will keep the motor cortex in my model simple.
**Two layers: 
**Input layer with 100 excitatory neurons and 25 inhibitory neurons.
**Output layer with 4 excitatory neurons only.
**Feedforward [IT to MI]: 25 E to 1 E, 225 E to 1 I, 25 I to 1 I, 
**Feedbackward [MI to IT]: 1 E to 9 E, 1 I to 25 E, No I to I
**Feedforward [MI to MO]: 400 E to 1 E (all to all by using probability of connection 1)
**Feedbackward [MO to MI]: 1 E to 121 E 
**There was a bug in connParams. Fixed now.

*19dec5
Analyzing...
  Cells: 17404
  Connections: 3189957 (183.29 per cell)
  Spikes: 770534 (4.43 Hz)
  Simulated time: 10.0 s; 1 workers
  Run time: 1838.15 s
Saving output as model_output.pkl ... 
Finished saving!
  Done; saving time = 995.82 s.
Plotting raster...
Saving figure data as RasterData.pkl ... 
  Done; plotting time = 98.44 s

Total time = 3753.03 s

*19dec6
**Sam suggested: May be should increase number of neurons in motor output layer (MO).
**Change MO from 2x2(4) to 10x10(100)
**Increase number of MO neurons from 4 to 100. Also changed the connectivity from MO to MI and MI to MO.
**Ran the simulation after changing MO as described above. Below are some runtime stats.
Analyzing...
  Cells: 17500
  Connections: 3212180 (183.55 per cell)
  Spikes: 762715 (4.36 Hz)
  Simulated time: 10.0 s; 1 workers
  Run time: 1810.90 s
  Done; saving time = 0.22 s.
Plotting raster...
Saving figure data as RasterData.pkl ... 
  Done; plotting time = 92.46 s
**Replace connections between Visual Cortex and Motor Cortex from STDP to Reward-based STDP.
**Keeping connections within every layer of Motor Cortex standard STDP.

*19dec10
**function playGame is modified to take a list of actions (5 actions) and returns rewards (5 rewards) associated with those actions.
**generate actions based on activity of MO.

*19dec11
**record firing rate of populations of neurons for 20 ms and use that rate to generate actions.
***100 MO neurons are divided into 50 neurons for Right-move(up) and 50 neurons for left-move(down) action.
***Those 50 neurons are further divided into groups of 10 neurons with each group respoisnble for 1 actions out of a sequence of 5 actions.

*19dec12
**Error detected in code in function playGame.

*19dec13
**Fixed the error and reran the simulation.
**action is generated based on activitiy of MO.
**RL code included---mainly modified from Salva's RL model. I don't understand rank and pc.broadcast related script.
**Still need to test the code.
**When i tested the code, i noticed that the firing rate of neuronal population generating actions was 0 most of the times.
**The reason might be that the connection weights reduced to 0 but have not checked that yet.
**So after discussing with Sam, I have made a list of things to test with the model as Below:
1. turn off the -1 and only use 0 or 1, what happens?
2. compare the results of full model simulation with the simulation after completing tuning off reward, punishment.
3. compare the results of full model simulation with the simulation after completely turning off STDP.
4. compare the results of full model simulation with the simulation after using RL-STDP for all synapses.
5. Can do 2 phase learning: phase 1 for STDP between visual layers. training is terminated when the firing rates become stable. Then use those weights for visual connections and train RL-STDP connections.
6. For different configurations, play with starting weights by increasing.
7. could try saving weights from visual learning, then restore them into the network, and continue from there with RL

Also for later he suggested: for the longer term should think about how to avoid "catastrophic forgetting" <- which is what that guy hananel mentioned - when certain neural networks have to learn > 1 thing, the new info/memory/task sometimes interferes with the previously learned. i think the bazhenov model/paper mentions that problem with a solution too (might involve homeostatic plasticity)

*19dec16
**implemented a function to record RL and NonRL weights separately and save at different cellular resolution.
**For every simulation, run for 100 sec, record weights for every 10th neuron.
***A few observations:
1. A lot of time steps, the firing rate of output neurons is 0HZ.
2. When the firing rate is 0 Hz, the action is "Don't move". So most of the times the racket is not moving.
***Find out why the firing rate of output layer is 0?
**While running the simulation, i could not save weights..... need debugging.

*19dec17
**Start debugging the code to save weights.
***I noticed that i was not passing argument sim to the function saveWeights(sim). I made this correction and now rerunning the simulation.
***Another problem detected: precision of t such that it never gives DT%1000==0


*19dec19
**On neurosim: Save the rasterdata and weights for bothRLandNonRL case.
**On neurosim: Run the simulation with only RL. i.e. turn RLon = 1 in STDPparams.
**Reduced model has 1102 neurons and 50260 connections (45.61 per cell)
***1. Reduced model with both RL and nonRL. Also no movement if firing rate of R neuron is equal to firing rate of L neuron.
***2. Reduced model with only RL for all neurons. Also no movement if firing rate of R neuron is equal to firing rate of L neuron.
***3. Reduced model with only RL for all neurons. Also no movement if firing rate of R neuron is equal to firing rate of L neuron.
***1.2. Reduced model with both RL and nonRL. Move randomly if firing rate of R neuron is equal to firing rate of L neuron.
***1.3. RECORD actions and rewards.

*19dec23
**Ran 100 sec simulation of reduced model with recording actions and rewards.
**actions were chosen based on the firing rate of MO neurons except when the firing rate was not decisive.

*19dec24
**run 100 sec simulation of reduced model with recording actions and rewards.
**assign reward = 1 if its 0. and 0 if its 1.

*19dec26
**run 100 sec simulation of reduced model with no actions but randomly generated rewards for RL.
***I want to see how does the network behave if the environment does not change after 1st episode (_NoAction.py). 
****e.g. how NonRL weights evolve: this will give some hint on whether change in NonRL is random or because of the image sequences.
****e.g. how RL weights evolve: this will be random or not.

*19dec30
**Problems:
***I told Sam about my frustration with the reward system in the game i.e. Model gets +1 only if the computer/other player makes a mistake. He argued that Model can get +1 also when It plays a good move. But I said that at this point model is not at that cognitive level to acieve that kind of smartness. Though I am not convinced that model will ever do that, can still hope for that.
***Sam was concerned about sudden drop of weights. I told him that is because of so many -1s. He suggested that I might mask -1 in the beginning and also slow down the change in weight due to -1.
***I will use a threshold for weights to activate punishment in RL.
***I will also decrease the change in weight due to punishment in RL.
***The other problem I observed was that the weights keep increasing even in the absense of new scenes. I think this will make difficult for model to learn much about the game environment as most of the activity is intrinsic (not evoked by the image).
***Sam wants me to save movies and also plot activity of output motor neurons.
**Before I do anything I discussed with Sam, I will implement my idea: GUIDE MODEL TO PLAY THE GAME BASED ON VECTOR ALGEBRA. (local dir: SMARTAgent_Reduced_12302019)
***GUIDE MODEL TO PLAY THE GAME BASED ON VECTOR ALGEBRA ---> Done. Running now.
***Change fixed number of trials to complete episodes i.e. number of trials until one of the players score 20. use 'done' and also record total episodes.

*19dec31
**The simulation I ran with VECTOR ALGEBRA using 2 different values of RLhebbwt (0.001 and 0.0001) were done. But I realized i made a mistake in RL signal i.e. i used 0 as +1.
**I will re run these simulations and also record activity of the motor neurons.

    #I don't understand the code below. Copied from Salva's RL model
    vec = h.Vector()
    if sim.rank == 0:
        rewards, epCount = sim.SMARTAgent.playGame(actions, epCount)
        critic = sum(rewards) # get critic signal (-1, 0 or 1)
        sim.pc.broadcast(vec.from_python([critic]), 0) # convert python list to hoc vector for broadcast data received from arm
    else: # other workers
        sim.pc.broadcast(vec, 0)
        critic = vec.to_python()[0] #till here I dont understand


**running reduced model with VECTOR ALEBRA and recording RLWeights, NonRLWeights, ActionsAndRewards, FiringRate of MotorNeurons in MO, binned 20 ms, ActionsPerEpisode

*20jan2
**I created a folder 'SMARTAgent_Reduced_12302019/Dec31' and saved the output files and analysis of the simulation ran on Dec31. The simulation was run using RLparams as
STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.0001, 'RLantiwt': -0.000,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0} ---> check RLdescription.txt
**create a new dir 'SMARTAgent_Reduced_12302019/Jan2_0' and run simulation with STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.001, 'RLantiwt': -0.000,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0} in folder 'SMARTAgent_Reduced_12302019'
**create a new dir 'SMARTAgent_Reduced_12302019/Jan2' and run another simulation with STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.001, 'RLantiwt': -0.0001,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}
**create a new dir 'SMARTAgent_Reduced_12302019/Jan2_1' and run another simulation with STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.01, 'RLantiwt': -0.0001,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}

*20jan3
**create a new dir 'SMARTAgent_Reduced_12302019/Jan3' and run another simulation with STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.001, 'RLantiwt': -0.00001,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}
**create a new dir 'SMARTAgent_Reduced_12302019/Jan3_1' and run another simulation with STDPparamsRL = {'hebbwt': 0.0001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.001, 'RLantiwt': -0.00001,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}
**Some ideas to test: 
***1. decrease inhibition (i think right now the activity is too sparse to have eligibitly established).
***2. To have better chances of learning, increase the window so that the chances for eligibility increases.
**create a new dir 'SMARTAgent_Reduced_12302019/Jan3_2' and run another simulation with same parameters as the simulation in Jan3_1. Additionally, only use +1 (line 734). Run for 10sec (line 589)
***Though there were only two +1 rewards, the weight kept increasing. So rerun the simulation with +1 and 0.

*20jan6
**creating 2 cell model 'SMARTAgent_2Cell_01062020' to understand and debug RL-model.

*20jan7
**running 2 cell model showed that:
1)RL weight change must be smaller or equal to the hebb weight change otherwise too many punishments reduce the weight to 0.
2)with RL = 1 and hebbwt assigned, the model implements hebb stdp and RL, both.
**running reduced model simulation with RL weight change adjusted to hebbwt in 'SMARTAgent_Reduced_12302019/Jan7' using 
STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.0000, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.00001, 'RLantiwt': -0.000,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0} 
**running reduced model simulation with RL weight change adjusted to hebbwt in 'SMARTAgent_Reduced_12302019/Jan7_1' using 
STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.0000, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.00005, 'RLantiwt': -0.000,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}
**running reduced model simulation with RL weight change adjusted to hebbwt in 'SMARTAgent_Reduced_12302019/Jan7_2' using
STDPparamsRL = {'hebbwt': 0.00001, 'antiwt':-0.0000, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.0001, 'RLantiwt': -0.000,
        'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}

*20jan9
**Do we need STDP between neurons in visual cortex? May be not. As there is an existing topological map, activation of neurons in V1 will encode spatial information about the visual space. and neurons in V4 and IT will encode associations between objects in visual scenes.
**run reduced model without nonRL-STDP.
**running reduced model simulation without nonRL-STDP in 'SMARTAgent_Reduced_12302019/Jan9'.
**D1R-expressing medium spiny neurons reinforce, while D2R-expressing MSNs encode punsihment. High concentrations of DA preferentially activate D1R-expressing direct pathway neurons, while low level of DA preferentially activates D2R expressing indirect pathway neurons.
**Kravitz and Kreitzer suggest LTP of the direct pathway and LTD of the indirect pathway mediates reinforcement, whereas LTP of the indirect pathway and LTD of direct pathway mediates punishment in substentia niagra compacta (SNc) neurons. 
**STDP works good.
**Reward based weight adjustment looks weird:
example 1:
t=500.600000 (BEFORE) tlaspre=499.200000, tlastpost=400.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=500.600000 (AFTER) tlaspre=499.200000, tlastpost=500.600000, flag=0.000000, w=-1.000000, deltaw=0.000000 

t= 520.0000000000497 - adjusting weights based on RL critic value: -1
t=520.000000, RL-hebb =0.001000, deltaw=-0.001000 
t=520.000000, RL-antihebb =0.000000, deltaw=-0.001000 
RL event: t = 520.000000 ms; reinf = -1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 500.600000; deltaw = -0.001000

example 2:
t=780.200000 (BEFORE) tlaspre=758.000000, tlastpost=779.000000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=780.200000 (AFTER) tlaspre=780.200000, tlastpost=779.000000, flag=0.000000, w=1.000000, deltaw=0.000000

t=798.800000 (BEFORE) tlaspre=780.200000, tlastpost=779.000000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=798.800000 (AFTER) tlaspre=798.800000, tlastpost=779.000000, flag=0.000000, w=1.000000, deltaw=0.000000 
eligibility established -- see above and now reward/punishment delivered at time 800ms -- see below

t= 800.0000000001133 - adjusting weights based on RL critic value: -1
t=800.000000, RL-hebb =0.001000, deltaw=-0.001000 
t=800.000000, RL-antihebb =-0.000200, deltaw=-0.000800 
RL event: t = 800.000000 ms; reinf = -1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 779.000000; deltaw = -0.000800

example 3:
t=801.200000 (BEFORE) tlaspre=798.800000, tlastpost=779.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=801.200000 (AFTER) tlaspre=798.800000, tlastpost=801.200000, flag=0.000000, w=-1.000000, deltaw=0.000000 

t=818.000000 (BEFORE) tlaspre=798.800000, tlastpost=801.200000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=818.000000 (AFTER) tlaspre=818.000000, tlastpost=801.200000, flag=0.000000, w=1.000000, deltaw=0.000000

t= 820.0000000001179 - adjusting weights based on RL critic value: -1
t=820.000000, RL-hebb =0.001000, deltaw=-0.001000 
t=820.000000, RL-antihebb =-0.000200, deltaw=-0.000800 
RL event: t = 820.000000 ms; reinf = -1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 801.200000; deltaw = -0.000800

example 4:
t=940.800000 (BEFORE) tlaspre=923.800000, tlastpost=881.000000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=940.800000 (AFTER) tlaspre=940.800000, tlastpost=881.000000, flag=0.000000, w=1.000000, deltaw=0.000000

t=944.800000 (BEFORE) tlaspre=940.800000, tlastpost=881.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=944.800000 (AFTER) tlaspre=940.800000, tlastpost=944.800000, flag=0.000000, w=-1.000000, deltaw=0.000000

t=959.400000 (BEFORE) tlaspre=940.800000, tlastpost=944.800000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=959.400000 (AFTER) tlaspre=959.400000, tlastpost=944.800000, flag=0.000000, w=1.000000, deltaw=0.000000

t= 960.0000000001497 - adjusting weights based on RL critic value: -1
t=960.000000, RL-hebb =0.001000, deltaw=-0.001000 
t=960.000000, RL-antihebb =0.000000, deltaw=-0.001000 
RL event: t = 960.000000 ms; reinf = -1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 944.800000; deltaw = -0.001000

example 5:

t=1059.400000 (BEFORE) tlaspre=1038.400000, tlastpost=1032.400000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=1059.400000 (AFTER) tlaspre=1038.400000, tlastpost=1059.400000, flag=0.000000, w=-1.000000, deltaw=0.000000 

t= 1080.0000000001132 - adjusting weights based on RL critic value: 1
t=1080.000000, RL-hebb =0.001000, deltaw=0.001000 
t=1080.000000, RL-antihebb =-0.000200, deltaw=0.000800 
RL event: t = 1080.000000 ms; reinf = 1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 1059.400000; deltaw = 0.000800


another example with more information: (both tlasthebbelig and tlastantielig)

t= 860.000000000127 - adjusting weights based on RL critic value: 1
t=860.000000, RL-hebb =0.001000, deltaw=0.001000 
t=860.000000, RL-antihebb =-0.000200, deltaw=0.000800 
RL event: t = 860.000000 ms; reinf = 1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 839.000000; tlastantielig = 840.400000; deltaw = 0.000800


**Still unsure how tlasthebbelig and tlastantielig set. Now I understand. It's explained below, with RLon = 1 and STDPon = 0
***eligibility is establised by comparing times of pre- and post-synaptic spikes.
*** the function receives w. if w>=0, its a presynaptic spike. if w<0 its postsynaptic spike.
*** if presynaptic spike at time t, it will look at the time of last postsynaptic spike. and therefore look for antihebbian eligibility.
*** if postsynaptic spike at time t, it will look at the time of last presynaptic spike and therefore look for hebbian eligibility.

**In the example below 2 spike events were printed

***spike event 1:
t=1253.600000 (BEFORE) tlaspre=1232.600000, tlastpost=1219.600000, tlasthebbelig=1219.600000, tlastantielig=-1.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=1253.600000 (AFTER) tlaspre=1232.600000, tlastpost=1253.600000, tlasthebbelig=1253.600000, tlastantielig=-1.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 

--at time 1253.6 ms, a postsynaptic spike occurs (because w = -1), therefore the mechanism compares time of last presynaptic spike which is 1232.6 ms
--the time difference between current postsynaptic spike and last presynaptic spike is 20 ms. 
--tpost-tpre (20 ms) is less than 100 ms therefore the eligibilty trace is activated at 1253.6 ms for hebbian plasticity.

***spike event 2:
t=1259.000000 (BEFORE) tlaspre=1232.600000, tlastpost=1253.600000, tlasthebbelig=1253.600000, tlastantielig=-1.000000, flag=0.000000, w=1.000000, deltaw=0.000000 
t=1259.000000 (AFTER) tlaspre=1259.000000, tlastpost=1253.600000, tlasthebbelig=1253.600000, tlastantielig=1259.000000, flag=0.000000, w=1.000000, deltaw=0.000000

--at time 1259 ms, a presynaptic spike accurs (because w = 1), therefore the mechanism compares time of last postsynaptic spike which is 1253.6 ms.
-- the time difference between current presynaptic spike and last postsynaptic spike is 5.4 ms.
--tpre-tpost (5.4 ms) is less than 10 ms therefore the eligibility trace is activated at 1259.0 ms for antihebbian plasticity.

*** a reward/punishment arrives at 1260 ms

t= 1259.9999999999495 - adjusting weights based on RL critic value: 1
t=1260.000000, RL-hebb =0.001000, deltaw=0.001000 
t=1260.000000, RL-antihebb =-0.000200, deltaw=0.000800 
RL event: t = 1260.000000 ms; reinf = 1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 1253.600000; tlastantielig = 1259.000000; deltaw = 0.000800

-- at time 1260 ms, eligibility for both hebbian and antihebbian were active (hebbian was active till 1353.6 ms and antihebb was active till 1359 ms).
-- since its a reward... it will implement positive reinforcement.
-- if it was a negative, both hebbian and antihebbian plasticities will be reversed.

** right now the window for hebbian plasticity is 100 ms where as antihebbian plasticity is 10 ms.
** the eligibility for both hebbian and anithebbian stays active for 100 ms.

**What happens when both RLon and STDPon are 1. See the example below.

t=47.200000 (AFTER) tlaspre=25.800000, tlastpost=47.200000, tlasthebbelig=47.200000, tlastantielig=-1.000000, flag=0.000000, w=-1.000000, deltaw=0.000000 
t=48.200000 (BEFORE) tlaspre=25.800000, tlastpost=47.200000, tlasthebbelig=47.200000, tlastantielig=-1.000000, flag=-1.000000, w=-1.000000, deltaw=0.000000 
Hebbian STDP event: t = 48.200000 ms; tlastpre = 25.800000; w = -1.000000; deltaw = 0.000118

t= 60.00000000000058 - adjusting weights based on RL critic value: -1
t=60.000000, RL-hebb =0.001000, deltaw=-0.001000 
t=60.000000, RL-antihebb =0.000000, deltaw=-0.001000 
RL event: t = 60.000000 ms; reinf = -1.000000; RLhebbwt = 0.001000; RLlenhebb = 100.000000; tlasthebbelig = 47.200000; tlastantielig = -1.000000; deltaw = -0.001000

**Sent 2 messages (Slack) to Sam on Jan10,2020 for discussion
Message 1:
While building SmartAgent model, i incorporated stdp.mod from RL_arm model 
(https://github.com/Neurosim-lab/netpyne/blob/development/examples/RL_arm/params.py)
and used the parameters: 
STDPparams = {'hebbwt': 0.00001, 'antiwt':-0.00001, 'wmax': 50, 'RLon': 1 , 'RLhebbwt': 0.001, 'RLantiwt': -0.000, \
    'tauhebb': 10, 'RLwindhebb': 50, 'useRLexp': 0, 'softthresh': 0, 'verbose':0}.
As earlier, i didn't carefully looked at all these parameters and didn't try to understand the meaning of each of these
parameters, while running a reduced verson of SmartAgent, I noticed a drop in the activity of postsynaptic neurons which 
were connected with presynaptic neurons using STDPparams as declared above. on dissecting these mechanisms, i found out:
1. It was using STDP-based and RL-based weight adjustments. It was because, by default STDPon is set to 1 and I didn't set STDPon to 0.
1.1. Before I further dive into the working of the mechanisms, i wonder why this choice was made? I am not against using this strategy 
but want to understand biological rationale behind this choice
2. The second set of parameters, i wondered about were 'RLhebbwt': 0.001, 'RLantiwt': -0.000.
2.1. RLhebbwt defines the change in weight, when RLon is 1 and hebbeligibilty is active. 
2.1.1 hebbeligibility is active for 100 ms if postsynaptic neuron fires within 50 ms ('RLwindhebb': 50) of presynaptic neuron. by default RLwindhebb is 10 ms.  
2.1.2 when there is a reward (+1), and hebbelegibility is active for less than 100 ms, the weight increases.
2.1.3 when there is a punishment (-1), and hebbelegibility is active for less than 100 ms, the weight decreases.
2.2. RLantiwt defines the change in weight, when RLon is 1 and antieligibility is active.
2.2.1. antieligibility is active for 100 ms if presynaptic neuron fires after postsynaptic neuron within 10 ms. (default: 'RLwindanti': 10) 
2.2.2. when there is a reward (+1), and antieligibility is active for less than 100 ms, the weight decreases.
2.2.3. when there is a punishment (-1), and antieligibility is active for less than 100 ms, the weight increases.
2.2.4. However, this mechanism is inactive because RLantiwt': -0.000.
2.2.5. So the question is why was it set to be inactive? Should I use it? What is the biological relevance of this mechanism?
3. The biggest issue in my model is that because the postsynaptic neurons are driven by presynaptic neurons, 
if the connection weight drops to 0, the postsynaptic neurons become inactive. Probably, need to have some baseline activity in the model for all neurons.

Message 2: 
Also just for discussion, if there is a topologically mapped neurons across multiple layers, i have been wondering why would we need STDP to encode spatial information. Same question from other side: how would STDP help encode visual information, all i can think (also see in the model) of is that it increase firing rate globally. I understand the basic concept but cant wrap my head around how this can be useful. Of course, i am trying both ways i.e. with STDP (adjustable weights) and without STDP (fixed weights) in Visual cortex.

*20jan13
**Things to do:
***Develop analytical tools/scripts to dissect the circuit to see how the activity is encoded hierarchically. V1 vs V4 vs IT etc
***Add noise to the Motor neurons so that these neurons can fire in the absence of synaptic input. This will give a chance to recover if the connection weights are depressed to 0.
***Remove STDP in Visual areas.
***what is the firing rate of neurons and their population in the absence of plasticity?
***How to have a better control over the racket movements in the aigame?

*20jan14
**add background noise firing rate with noise to excitatory cells.
netParams.stimSourceParams['ebkg'] = {'type': 'NetStim', 'rate': 5, 'noise': 0.3}
netParams.stimTargetParams['ebkg->all'] = {'source': 'ebkg', 'conds': {'cellType': ['EV1','EV4','EIT', 'EMI', 'EMO']}, 'weight': 0.01, 'delay': 'max(1, normal(5,2))', 'synMech': 'AMPA'}  
**In folder 'SMARTAgent_Reduced_01142020/AllConns0' set weights of all connections to 0 to see the background activity of the network i.e. R will fire based on the input and E cells will fire based on ebkg and i cells will fire based on bkg.
**In folder 'SMARTAgent_Reduced_01142020/AllConns0ExceptReccurent' set wights of all connections to 0 except recurrent connections and background activity as in the above case.
**In folder 'SMARTAgent_Reduced_01142020/AllConns0ExceptFeedForwardExcit' set weights of all connections to 0 except feedforward excitatory connections and background activity.
**When compared AllConns0 with AllConns0ExceptRecurrent, found no difference. Rerunning ALlConns0ExceptRecurrent with higher connection strength (0.0001 --> 0.001) 

*20jan15
**For 'SMARTAgent_Reduced_01142020/AllConns0ExceptFeedForwardExcit', we had used weight of 0.002 and had run simulation for 100 sec.
*** Comparing the firing rate to the baseline, it seemed that 0.002 was high. 
***So for comparison at different values for weights, i reran the simulation for 10 sec with 0.002.
***Running simulation for 10 sec with weight of 0.001 and 0.0005

*20jan16
**worked on plotting neural activity to show spatial patterns of neurons activated in diffent layers.
**finished in matlab and started implementing in python.

*20jan17
**finished plotting neural activity to show spatial patterns of neurons activated in different layers.
**found that: 
1) The activity didnt propagate across the layers faithfully. Rather only noise was seen across layers.
2) Sam thinks that noise is good however, the activity should be propagated across layers faithfully.
2.1) For propagation, he suggested may be we should increase the strength of the connections.
2.2) I think the limitation is time step for playing game i.e. 20 ms. it allows only to detect 50Hz stimulation, any input driving lower frequency will not bre represented.
2.2.1) Solution could be shifting either the activiation curve to increase firing rates or increase the time step for playing game. If we increase step to 100 ms. this will allow to encode atleast 10Hz input.
3) Also for comparison, Sam suggested that we should also plot the actual input image. 
