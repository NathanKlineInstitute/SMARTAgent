
*19nov27
** trying on laptop

do not have netpyne on laptop

python3
import netpyne

pip3 install netpyne

Traceback (most recent call last):
  File "/usr/bin/pip3", line 9, in <module>
    from pip import main
ImportError: cannot import name 'main'

what's wrong with pip3?

https://stackoverflow.com/questions/49836676/error-after-upgrading-pip-cannot-import-name-main

sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall

pip3 install netpyne

hmm, needs newer version of python (>= 3.6 ), only have python 3.5 on laptop ...

will try on neurosim ... 

python3
import netpyne
netpyne.__version__ # '0.9.3.1'

pip3 install gym --user
pip3 install atari-py --user

compile:
nrnivmodl

for running on 1 node:
py3env
python3 trainSmartAgent.py

mpirun -n 16 python trainSmartAgent.py

was able to run with 1 core, took 2.5 GB with 500 ms interval for saving weights and 1000 ms simulation
will need to record from only a small fraction of the cells

*20feb13
** make new branch to avoid conflict with haroon's work

git branch samn
git checkout samn
git add snnotes.dol
git commit -m 'new branch for samn test'
git push origin samn

make an alias for that: gpushsamn

** try compile and then run 

nrnivmodl

mpirun -n 16 python trainSmartAgent.py

myrun

mpirun -n 16 python trainSmartAgent.py

even after calling py3env to set the environment to use anaconda ... 
it's showing many different pong windows ... should only be 1 window  (this was run on zn)

aigame.py loads the gym environment with the pong game
where is aigame.py called from?

trainSmartAgent.py is the main sim setup
it imports SMARTAgent from aigame

hmm, not running it properly ...

mpiexec -n 16 python -mpi trainSmartAgent.py

*20feb24
** HA fixed the MPI issues
** set env.frameskip to a constant value on environment init to avoid random frameskip in a range
** setup code for some more flexibility

can use json for config file

*20feb25
** adjust architecture add direct V1 -> M popoulations

that way M has higher resolution visual information
and M still receives the lower resolution visual information from V2, IT as well ...

** simple test - reward for moving up, punish for moving down

does it produce expected behavior?

myrun 16

python
import numpy as np
from pylab import *
ion()
d = np.loadtxt('ActionsRewards.txt')
len(np.where(d[:,1]==3)[0]) # 232
len(np.where(d[:,1]==4)[0]) # 246
len(np.where(d[:,1]==1)[0]) # 272

plot(d[:,0],d[:,1],'ko')
hist(d[:,1])

to test if it's working just check the RL weights onto ML vs MR; weights onto ML neurons should
increase, while weights onto MR should decrease ...  if that's not happening, something is wrong ...

*20feb26
** looking at the output weights for the fake training task

myrun 1
quit()

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')
awt.shape # (21999, 5) # columns are time, preid, postid, stdptype, weight

plot(awt[:,0],awt[:,4],'ko')
savefig('gif/20feb26_a0.png')

need to know cell types ...

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
len(pdf) # 21999

pdfs = pdf[(pdf.postid==1184) & (pdf.stdptype==1)]
len(pdfs) # 879
plot(pdfs.time,pdfs.weight,'r')
savefig('gif/20feb26_a1.png') # looks incorrect ?? does it go up and down or are those two different synapses?

min(pdfs.preid),max(pdfs.preid) # (403.0, 924.0)
yeah, two preids ... and they're differnet because different source populations ...

pdfs = pdf[(pdf.postid==1184) & (pdf.stdptype==1) & (pdf.preid==403)]
len(pdfs) # 20
plot(pdfs.time,pdfs.weight,'b')
savefig('gif/20feb26_a2.png')
ok, that weight is increasing gradually ... but is that the ML or MR output population?

ID 1159 through 1183 (inclusive) are the ML neurons? (/u/samn/SMARTAgent/trainSmartAgent.py:754)

pdfs = pdf[(pdf.postid==1159) & (pdf.stdptype==1)]
len(pdfs) # 0

pdfs = pdf[(pdf.postid==1160) & (pdf.stdptype==1)]
len(pdfs) # 0

pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]
len(pdfs) # 0

hmm, are any of the ML weights getting saved??

note that this was run with a single core ...

ah, a bug in new code ...

fix, rerun ...

myrun 1

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')
awt.shape # (22000, 5) # columns are time, preid, postid, stdptype, weight

plot(awt[:,0],awt[:,4],'ko')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]
len(pdfs) # 11000

should save types in the file ... 

plot(pdfs.time,pdfs.weight,'b')

myrun 1
sim.net.cells[0].tags['pop'] # 'R'

sim.net.cells[1184].tags['pop'] # 'MR'
sim.net.cells[1159].tags['pop'] # 'ML'

*20feb27
** continue debugging

to get the network/cell info use this:
simConfig.savePickle = True            # Save params, network and sim output to pickle file

myrun 1

from pylab import *
savefig('gif/20feb27_rast_a0.png')

simConfig.filename = 'data/simConfig'
sim.saveFolder = 'data'

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]

import pickle
simConfig = pickle.load(open('data/simConfig.pkl','rb'))
simConfig.keys() # dict_keys(['netpyne_version', 'netpyne_changeset', 'net', 'simConfig', 'simData'])

simConfig['net'].keys() # dict_keys(['params', 'cells', 'pops'])
simConfig['net']['pops'].keys() # odict_keys(['R', 'V1', 'V4', 'IT', 'IR', 'IV1', 'IV4', 'IIT', 'ML', 'MR'])
simConfig['net']['pops']['MR'].keys() # dict_keys(['tags', 'cellGids'])
simConfig['net']['pops']['MR']['cellGids'] # [1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208]
simConfig['net']['pops']['ML']['cellGids'] # [1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183]

ok, good ...

dstartidx = {p:simConfig['net']['pops'][p]['cellGids'][0] for p in simConfig['net']['pops'].keys()}
dendidx = {p:simConfig['net']['pops'][p]['cellGids'][-1] for p in simConfig['net']['pops'].keys()}

dstartidx # {'R': 0, 'V1': 400, 'V4': 800, 'IT': 900, 'IR': 925, 'IV1': 1025, 'IV4': 1125, 'IIT': 1150, 'ML': 1159, 'MR': 1184}
dendidx # {'R': 399, 'V1': 799, 'V4': 899, 'IT': 924, 'IR': 1024, 'IV1': 1124, 'IV4': 1149, 'IIT': 1158, 'ML': 1183, 'MR': 1208}

pdfs = pdf[(pdf.postid>=dstartidx['ML']) & (pdf.postid<=dendidx['ML']) & (pdf.preid>=dstartidx['V1']) & (pdf.preid<=dendidx['V1']) & (pdf.stdptype==1)]
len(pdfs) # 4000

plot(pdfs.time,pdfs.weight,'bo')

savefig('gif/20feb27_wghts_a1.png')

pdfs = pdf[(pdf.postid>=dstartidx['MR']) & (pdf.postid<=dendidx['MR']) & (pdf.preid>=dstartidx['V1']) & (pdf.preid<=dendidx['V1']) & (pdf.stdptype==1)]
len(pdfs) # 4000

plot(pdfs.time,pdfs.weight,'ro')

savefig('gif/20feb27_wghts_a2.png')

so both ML and MR weights are increasing - that's incorrect

checking if recording the synaptic weights into sim.simData['synweights'] will work
with netpyne gathering the info across nodes automatically ...

python3
import numpy as np
from pylab import *
import pickle

simConfig = pickle.load(open('data/simConfig.pkl','rb'))
simConfig.keys() # dict_keys(['netpyne_version', 'netpyne_changeset', 'net', 'simConfig', 'simData'])
simConfig['simData'].keys() # dict_keys(['spkt', 'spkid', 'V_soma', 't', 'synweights', 'avgRate'])
len(simConfig['simData']['synweights']) # 22000
simConfig['simData']['synweights'][0] # [99.9999999999986, 0.0025, 1184, 900, 1]

and that was when running with 1 node ... try again with > 1 to see if same

myrun 16

Traceback (most recent call last):
  File "sim.py", line 988, in <module>
    sim.gatherData() # gather data from different nodes
  File "/usr/site/python/netpyne/netpyne/sim/gather.py", line 165, in gatherData
    sim.allSimData[key].update(val)           # update simData dicts which are not Vectors
ValueError: dictionary update sequence element #0 has length 5; 2 is required

change sim.simData['synweights'] to a dict (with key 0 pointing to a list of lists)

type(sim.simData['synweights']) # <class 'dict'>
sim.simData['synweights'][0][0] # [99.9999999999986, 0.0025, 1184, 912, 1]
len(sim.simData['synweights'][0]) # 1320
len(sim.allSimData['synweights'][0]) # 1320
sim.allSimData['synweights'].keys() # dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
len(sim.allSimData['synweights'][15]) # 1320
sum([len(sim.allSimData['synweights'][i]) for i in range(16)]) # 22000

import pickle
simConfig = pickle.load(open('data/simConfig.pkl','rb'))
len(simConfig['simData']['synweights'][0]) # 1320
simConfig['simData']['synweights'].keys() # dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
simConfig['simData']['synweights'][0][0] # [99.9999999999986, 0.0025, 1184, 912, 1]

ion()
awt = np.loadtxt('data/AdjustableWeights.txt')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])


ok, good ...

dstartidx = {p:simConfig['net']['pops'][p]['cellGids'][0] for p in simConfig['net']['pops'].keys()}
dendidx = {p:simConfig['net']['pops'][p]['cellGids'][-1] for p in simConfig['net']['pops'].keys()}

testing basic mechanism again , with RL exp off and fake up rule ... 

myrun 16

plw

savefig('gif/20feb27_rewards_wghts_a3.png')

seems to generally work, once separated out the population specific projections (V1->ML and V1->MR) ...
but not clear why V1->MR is getting reinforced when MR produces down moves ...

could be that MR getting reinforced since V1 projects to both MR and ML and V1->MR synapses are tagged
within the interval when V1->ML synapses are tagged for reward ...

try cutting off some of the higher connections connections and see what happens ... e.g. no V1 -> MR
and no V4, IT -> ML or -> MR; and may need to set wbase to 0 as well ...

myrun 16

now for some reason ML never firing ... ??? no, ML firing but MR not firing (that was the test)
but mostly only producing move 3 (down), instead of 4 (up) ... why?

savefig('gif/20feb27_rewards_wghts_a4.png')
savefig('gif/20feb27_rewards_wghts_a5.png')

hmm, had the rule backwards:
            if F_R1>F_L1:
                actions.append(dconf['moves']['UP']) #UP
            elif F_R1<F_L1:
                actions.append(dconf['moves']['DOWN']) # Down
            else:
            actions.append(dconf['moves']['NOMOVE']) # No move

R produces up, L produces down ...

ok, fixing that, now V->MR weights go up as they should (but now have turned off inputs to ML)
savefig('gif/20feb27_rewards_wghts_a6.png')

so, turned back on the inputs to ML to see if -> MR weights still go up with time ...

savefig('gif/20feb27_raster_a7.png')

savefig('gif/20feb27_rewards_weights_a8.png')

it does look like the V -> MR weights increase more than the V -> ML weights, which
is correct, but the V -> ML weights still seem to correlate with the V -> MR weights, and go up in parallel
perhaps, as long as overall more of the correct moves are made, it doesn't matter if the V -> ML weights
are increased too ... ?

what is the move command as a function of time? more correct moves later on compared to earlier?

pad = pd.DataFrame(actreward,columns=['time','action','reward'])

figure(); pads = pad[pad.action==3]; plot(pads.time,pads.action,'bo'); pads = pad[pad.action==4]; plot(pads.time,pads.action,'ro')

#
ldown,lup = [],[]
ltt = linspace(0,99,100)
for tt in ltt:
  pads = pad[(pad.action==3) & (pad.time>=tt*1e3) & (pad.time<=(tt+1)*1e3)]
  ldown.append(len(pads))
  pads = pad[(pad.action==4) & (pad.time>=tt*1e3) & (pad.time<=(tt+1)*1e3)]
  lup.append(len(pads))

clf(); plot(ltt,ldown,'r',linewidth=4); plot(ltt,lup,'b',linewidth=4); xlabel('Time (s)'); ylabel('Move frequency (Hz)');
import matplotlib.patches as mpatches
lpatch = [mpatches.Patch(color=c,label=s) for c,s in zip(['b','r'],['Up','Down'])]
ax=gca()
ax.legend(handles=lpatch,handlelength=1)

savefig('gif/20feb27_action_freq_a9.png')

most of the time, up moves have higher rate than down moves ...

see if it's reversed when using the down fake rule ...

ok, using sim.json sim:name to specify simulation name so can save output files for different sim in data ...

also may as well adjust plotWeights to draw actions in top panel ...? suppose only useful when using fake rule ... 
otherwise up/down rates not so important ...  

sim name is 20feb27_FakeDownRule_B0_

myrun 16

  Cells: 1209
  Connections: 27879 (23.06 per cell)
  Spikes: 173390 (1.43 Hz)
  Simulated time: 100.0 s; 16 workers
  Run time: 2309.54 s

savefig('gif/20feb27_raster_b0.png')

more spikes at bottom, where they should be ... ML has higher firing rate (ML produces down move), much more than in previous example where MR had
slightly higher firing rate than ML  ... maybe a bug somewhere? why the difference?

and now check the weights and action freqs ... 

plw

savefig('gif/20feb27_rewards_weights_b1.png')

#
ldown,lup = [],[]
ltt = linspace(0,99,100)
for tt in ltt:
  actrewards = actreward[(actreward.action==3) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  ldown.append(len(actrewards))
  actrewards = actreward[(actreward.action==4) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  lup.append(len(actrewards))

figure(); plot(ltt,ldown,'r',linewidth=4); plot(ltt,lup,'b',linewidth=4); xlabel('Time (s)'); ylabel('Move frequency (Hz)');
import matplotlib.patches as mpatches
lpatch = [mpatches.Patch(color=c,label=s) for c,s in zip(['b','r'],['Up','Down'])]
ax=gca()
ax.legend(handles=lpatch,handlelength=1)

savefig('gif/20feb27_action_freq_b2.png')

so, there are more down actions generally ..., particularly as the sim progresses ... maybe less consistent
than in previous FAKE MOVEUP test ...

not clear why would get diff results for the two fake rule tests ... one up and one down ... maybe some bug,
or some runaway effect ... 

could run another test to see if teach net to hold paddle still ... leading to suppression of ML and MR ...

ok, will try that ... with this sim name: 20feb27_FakeStayRule_C0_
and RLFakeStayRule == 1

note that any move up or down (in the 5 action block) is a penalty and any stay command is opposite (for the critic signal)... 

myrun 16

  Cells: 1209
  Connections: 27879 (23.06 per cell)
  Spikes: 173771 (1.44 Hz)
  Simulated time: 100.0 s; 16 workers
  Run time: 2286.23 s
Saving output as data/20feb27_FakeStayRule_C0_simConfig.pkl ... 
Finished saving!
Done; saving time = 11.18 s.

savefig('gif/20feb27_FakeStayRule_C0_raster.png')
lower firing rates for MR, ML ... next, look at behavior and weights...

plw

savefig('gif/20feb27_FakeStayRule_C0_rewards_weights_c0.png')

both sets of weights stay close to baseline ... 
would have expected both sets to decrease towards 0

savefig('gif/20feb27_FakeStayRule_C0_action_freq_c0.png')

well, at least neither action dominates here ...

