
*19nov27
** trying on laptop

do not have netpyne on laptop

python3
import netpyne

pip3 install netpyne

Traceback (most recent call last):
  File "/usr/bin/pip3", line 9, in <module>
    from pip import main
ImportError: cannot import name 'main'

what's wrong with pip3?

https://stackoverflow.com/questions/49836676/error-after-upgrading-pip-cannot-import-name-main

sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall

pip3 install netpyne

hmm, needs newer version of python (>= 3.6 ), only have python 3.5 on laptop ...

will try on neurosim ... 

python3
import netpyne
netpyne.__version__ # '0.9.3.1'

pip3 install gym --user
pip3 install atari-py --user

compile:
nrnivmodl

for running on 1 node:
py3env
python3 trainSmartAgent.py

mpirun -n 16 python trainSmartAgent.py

was able to run with 1 core, took 2.5 GB with 500 ms interval for saving weights and 1000 ms simulation
will need to record from only a small fraction of the cells

*20feb13
** make new branch to avoid conflict with haroon's work

git branch samn
git checkout samn
git add snnotes.dol
git commit -m 'new branch for samn test'
git push origin samn

make an alias for that: gpushsamn

** try compile and then run 

nrnivmodl

mpirun -n 16 python trainSmartAgent.py

myrun

mpirun -n 16 python trainSmartAgent.py

even after calling py3env to set the environment to use anaconda ... 
it's showing many different pong windows ... should only be 1 window  (this was run on zn)

aigame.py loads the gym environment with the pong game
where is aigame.py called from?

trainSmartAgent.py is the main sim setup
it imports SMARTAgent from aigame

hmm, not running it properly ...

mpiexec -n 16 python -mpi trainSmartAgent.py

*20feb24
** HA fixed the MPI issues
** set env.frameskip to a constant value on environment init to avoid random frameskip in a range
** setup code for some more flexibility

can use json for config file

*20feb25
** adjust architecture add direct V1 -> M popoulations

that way M has higher resolution visual information
and M still receives the lower resolution visual information from V2, IT as well ...

** simple test - reward for moving up, punish for moving down

does it produce expected behavior?

myrun 16

python
import numpy as np
from pylab import *
ion()
d = np.loadtxt('ActionsRewards.txt')
len(np.where(d[:,1]==3)[0]) # 232
len(np.where(d[:,1]==4)[0]) # 246
len(np.where(d[:,1]==1)[0]) # 272

plot(d[:,0],d[:,1],'ko')
hist(d[:,1])

to test if it's working just check the RL weights onto ML vs MR; weights onto ML neurons should
increase, while weights onto MR should decrease ...  if that's not happening, something is wrong ...

*20feb26
** looking at the output weights for the fake training task

myrun 1
quit()

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')
awt.shape # (21999, 5) # columns are time, preid, postid, stdptype, weight

plot(awt[:,0],awt[:,4],'ko')
savefig('gif/20feb26_a0.png')

need to know cell types ...

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
len(pdf) # 21999

pdfs = pdf[(pdf.postid==1184) & (pdf.stdptype==1)]
len(pdfs) # 879
plot(pdfs.time,pdfs.weight,'r')
savefig('gif/20feb26_a1.png') # looks incorrect ?? does it go up and down or are those two different synapses?

min(pdfs.preid),max(pdfs.preid) # (403.0, 924.0)
yeah, two preids ... and they're differnet because different source populations ...

pdfs = pdf[(pdf.postid==1184) & (pdf.stdptype==1) & (pdf.preid==403)]
len(pdfs) # 20
plot(pdfs.time,pdfs.weight,'b')
savefig('gif/20feb26_a2.png')
ok, that weight is increasing gradually ... but is that the ML or MR output population?

ID 1159 through 1183 (inclusive) are the ML neurons? (/u/samn/SMARTAgent/trainSmartAgent.py:754)

pdfs = pdf[(pdf.postid==1159) & (pdf.stdptype==1)]
len(pdfs) # 0

pdfs = pdf[(pdf.postid==1160) & (pdf.stdptype==1)]
len(pdfs) # 0

pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]
len(pdfs) # 0

hmm, are any of the ML weights getting saved??

note that this was run with a single core ...

ah, a bug in new code ...

fix, rerun ...

myrun 1

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')
awt.shape # (22000, 5) # columns are time, preid, postid, stdptype, weight

plot(awt[:,0],awt[:,4],'ko')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]
len(pdfs) # 11000

should save types in the file ... 

plot(pdfs.time,pdfs.weight,'b')

myrun 1
sim.net.cells[0].tags['pop'] # 'R'

sim.net.cells[1184].tags['pop'] # 'MR'
sim.net.cells[1159].tags['pop'] # 'ML'

*20feb27
** continue debugging

to get the network/cell info use this:
simConfig.savePickle = True            # Save params, network and sim output to pickle file

myrun 1

from pylab import *
savefig('gif/20feb27_rast_a0.png')

simConfig.filename = 'data/simConfig'
sim.saveFolder = 'data'

python3
import numpy as np
from pylab import *
ion()
awt = np.loadtxt('data/AdjustableWeights.txt')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])
pdfs = pdf[(pdf.postid>=1159) & (pdf.postid<=1183) & (pdf.stdptype==1)]

import pickle
simConfig = pickle.load(open('data/simConfig.pkl','rb'))
simConfig.keys() # dict_keys(['netpyne_version', 'netpyne_changeset', 'net', 'simConfig', 'simData'])

simConfig['net'].keys() # dict_keys(['params', 'cells', 'pops'])
simConfig['net']['pops'].keys() # odict_keys(['R', 'V1', 'V4', 'IT', 'IR', 'IV1', 'IV4', 'IIT', 'ML', 'MR'])
simConfig['net']['pops']['MR'].keys() # dict_keys(['tags', 'cellGids'])
simConfig['net']['pops']['MR']['cellGids'] # [1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208]
simConfig['net']['pops']['ML']['cellGids'] # [1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183]

ok, good ...

dstartidx = {p:simConfig['net']['pops'][p]['cellGids'][0] for p in simConfig['net']['pops'].keys()}
dendidx = {p:simConfig['net']['pops'][p]['cellGids'][-1] for p in simConfig['net']['pops'].keys()}

dstartidx # {'R': 0, 'V1': 400, 'V4': 800, 'IT': 900, 'IR': 925, 'IV1': 1025, 'IV4': 1125, 'IIT': 1150, 'ML': 1159, 'MR': 1184}
dendidx # {'R': 399, 'V1': 799, 'V4': 899, 'IT': 924, 'IR': 1024, 'IV1': 1124, 'IV4': 1149, 'IIT': 1158, 'ML': 1183, 'MR': 1208}

pdfs = pdf[(pdf.postid>=dstartidx['ML']) & (pdf.postid<=dendidx['ML']) & (pdf.preid>=dstartidx['V1']) & (pdf.preid<=dendidx['V1']) & (pdf.stdptype==1)]
len(pdfs) # 4000

plot(pdfs.time,pdfs.weight,'bo')

savefig('gif/20feb27_wghts_a1.png')

pdfs = pdf[(pdf.postid>=dstartidx['MR']) & (pdf.postid<=dendidx['MR']) & (pdf.preid>=dstartidx['V1']) & (pdf.preid<=dendidx['V1']) & (pdf.stdptype==1)]
len(pdfs) # 4000

plot(pdfs.time,pdfs.weight,'ro')

savefig('gif/20feb27_wghts_a2.png')

so both ML and MR weights are increasing - that's incorrect

checking if recording the synaptic weights into sim.simData['synweights'] will work
with netpyne gathering the info across nodes automatically ...

python3
import numpy as np
from pylab import *
import pickle

simConfig = pickle.load(open('data/simConfig.pkl','rb'))
simConfig.keys() # dict_keys(['netpyne_version', 'netpyne_changeset', 'net', 'simConfig', 'simData'])
simConfig['simData'].keys() # dict_keys(['spkt', 'spkid', 'V_soma', 't', 'synweights', 'avgRate'])
len(simConfig['simData']['synweights']) # 22000
simConfig['simData']['synweights'][0] # [99.9999999999986, 0.0025, 1184, 900, 1]

and that was when running with 1 node ... try again with > 1 to see if same

myrun 16

Traceback (most recent call last):
  File "sim.py", line 988, in <module>
    sim.gatherData() # gather data from different nodes
  File "/usr/site/python/netpyne/netpyne/sim/gather.py", line 165, in gatherData
    sim.allSimData[key].update(val)           # update simData dicts which are not Vectors
ValueError: dictionary update sequence element #0 has length 5; 2 is required

change sim.simData['synweights'] to a dict (with key 0 pointing to a list of lists)

type(sim.simData['synweights']) # <class 'dict'>
sim.simData['synweights'][0][0] # [99.9999999999986, 0.0025, 1184, 912, 1]
len(sim.simData['synweights'][0]) # 1320
len(sim.allSimData['synweights'][0]) # 1320
sim.allSimData['synweights'].keys() # dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
len(sim.allSimData['synweights'][15]) # 1320
sum([len(sim.allSimData['synweights'][i]) for i in range(16)]) # 22000

import pickle
simConfig = pickle.load(open('data/simConfig.pkl','rb'))
len(simConfig['simData']['synweights'][0]) # 1320
simConfig['simData']['synweights'].keys() # dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
simConfig['simData']['synweights'][0][0] # [99.9999999999986, 0.0025, 1184, 912, 1]

ion()
awt = np.loadtxt('data/AdjustableWeights.txt')

import pandas as pd
pdf = pd.DataFrame(awt,columns=['time','preid','postid','stdptype','weight'])


ok, good ...

dstartidx = {p:simConfig['net']['pops'][p]['cellGids'][0] for p in simConfig['net']['pops'].keys()}
dendidx = {p:simConfig['net']['pops'][p]['cellGids'][-1] for p in simConfig['net']['pops'].keys()}

testing basic mechanism again , with RL exp off and fake up rule ... 

myrun 16

plw

savefig('gif/20feb27_rewards_wghts_a3.png')

seems to generally work, once separated out the population specific projections (V1->ML and V1->MR) ...
but not clear why V1->MR is getting reinforced when MR produces down moves ...

could be that MR getting reinforced since V1 projects to both MR and ML and V1->MR synapses are tagged
within the interval when V1->ML synapses are tagged for reward ...

try cutting off some of the higher connections connections and see what happens ... e.g. no V1 -> MR
and no V4, IT -> ML or -> MR; and may need to set wbase to 0 as well ...

myrun 16

now for some reason ML never firing ... ??? no, ML firing but MR not firing (that was the test)
but mostly only producing move 3 (down), instead of 4 (up) ... why?

savefig('gif/20feb27_rewards_wghts_a4.png')
savefig('gif/20feb27_rewards_wghts_a5.png')

hmm, had the rule backwards:
            if F_R1>F_L1:
                actions.append(dconf['moves']['UP']) #UP
            elif F_R1<F_L1:
                actions.append(dconf['moves']['DOWN']) # Down
            else:
            actions.append(dconf['moves']['NOMOVE']) # No move

R produces up, L produces down ...

ok, fixing that, now V->MR weights go up as they should (but now have turned off inputs to ML)
savefig('gif/20feb27_rewards_wghts_a6.png')

so, turned back on the inputs to ML to see if -> MR weights still go up with time ...

savefig('gif/20feb27_raster_a7.png')

savefig('gif/20feb27_rewards_weights_a8.png')

it does look like the V -> MR weights increase more than the V -> ML weights, which
is correct, but the V -> ML weights still seem to correlate with the V -> MR weights, and go up in parallel
perhaps, as long as overall more of the correct moves are made, it doesn't matter if the V -> ML weights
are increased too ... ?

what is the move command as a function of time? more correct moves later on compared to earlier?

pad = pd.DataFrame(actreward,columns=['time','action','reward'])

figure(); pads = pad[pad.action==3]; plot(pads.time,pads.action,'bo'); pads = pad[pad.action==4]; plot(pads.time,pads.action,'ro')

#
ldown,lup = [],[]
ltt = linspace(0,99,100)
for tt in ltt:
  pads = pad[(pad.action==3) & (pad.time>=tt*1e3) & (pad.time<=(tt+1)*1e3)]
  ldown.append(len(pads))
  pads = pad[(pad.action==4) & (pad.time>=tt*1e3) & (pad.time<=(tt+1)*1e3)]
  lup.append(len(pads))

clf(); plot(ltt,ldown,'r',linewidth=4); plot(ltt,lup,'b',linewidth=4); xlabel('Time (s)'); ylabel('Move frequency (Hz)');
import matplotlib.patches as mpatches
lpatch = [mpatches.Patch(color=c,label=s) for c,s in zip(['b','r'],['Up','Down'])]
ax=gca()
ax.legend(handles=lpatch,handlelength=1)

savefig('gif/20feb27_action_freq_a9.png')

most of the time, up moves have higher rate than down moves ...

see if it's reversed when using the down fake rule ...

ok, using sim.json sim:name to specify simulation name so can save output files for different sim in data ...

also may as well adjust plotWeights to draw actions in top panel ...? suppose only useful when using fake rule ... 
otherwise up/down rates not so important ...  

sim name is 20feb27_FakeDownRule_B0_

myrun 16

  Cells: 1209
  Connections: 27879 (23.06 per cell)
  Spikes: 173390 (1.43 Hz)
  Simulated time: 100.0 s; 16 workers
  Run time: 2309.54 s

savefig('gif/20feb27_raster_b0.png')

more spikes at bottom, where they should be ... ML has higher firing rate (ML produces down move), much more than in previous example where MR had
slightly higher firing rate than ML  ... maybe a bug somewhere? why the difference?

and now check the weights and action freqs ... 

plw

savefig('gif/20feb27_rewards_weights_b1.png')

#
ldown,lup = [],[]
ltt = linspace(0,99,100)
for tt in ltt:
  actrewards = actreward[(actreward.action==3) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  ldown.append(len(actrewards))
  actrewards = actreward[(actreward.action==4) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  lup.append(len(actrewards))

figure(); plot(ltt,ldown,'r',linewidth=4); plot(ltt,lup,'b',linewidth=4); xlabel('Time (s)'); ylabel('Move frequency (Hz)');
import matplotlib.patches as mpatches
lpatch = [mpatches.Patch(color=c,label=s) for c,s in zip(['b','r'],['Up','Down'])]
ax=gca()
ax.legend(handles=lpatch,handlelength=1)

savefig('gif/20feb27_action_freq_b2.png')

so, there are more down actions generally ..., particularly as the sim progresses ... maybe less consistent
than in previous FAKE MOVEUP test ...

not clear why would get diff results for the two fake rule tests ... one up and one down ... maybe some bug,
or some runaway effect ... 

could run another test to see if teach net to hold paddle still ... leading to suppression of ML and MR ...

ok, will try that ... with this sim name: 20feb27_FakeStayRule_C0_
and RLFakeStayRule == 1

note that any move up or down (in the 5 action block) is a penalty and any stay command is opposite (for the critic signal)... 

myrun 16

  Cells: 1209
  Connections: 27879 (23.06 per cell)
  Spikes: 173771 (1.44 Hz)
  Simulated time: 100.0 s; 16 workers
  Run time: 2286.23 s
Saving output as data/20feb27_FakeStayRule_C0_simConfig.pkl ... 
Finished saving!
Done; saving time = 11.18 s.

savefig('gif/20feb27_FakeStayRule_C0_raster.png')
lower firing rates for MR, ML ... next, look at behavior and weights...

plw

savefig('gif/20feb27_FakeStayRule_C0_rewards_weights_c0.png')

both sets of weights stay close to baseline ... 
would have expected both sets to decrease towards 0

savefig('gif/20feb27_FakeStayRule_C0_action_freq_c0.png')

well, at least neither action dominates here ...

*20feb28
** continue 

should make it easier to load the output data ... maybe consolidate plotting funcs too ...

question about architecture - how much of the higher level areas do we need/want? and should
allow easier scaling of the simulation in case need larger-sized populations  ... 

homeostatic plasticity will likely be needed - the weights were increasing, and could push
the net to epilepsy ...

try full architecture with the fake rules ... possible that higher order connections are not a problem ...

myrun 24

looks like M population might be too small ... ML and MR neurons should receive largely overlapping inputs from V1
because of the much higher number of V1 (400) compared to ML (25) and MR (25) neurons , with the high convergence (16)
this might be reason see highly synchronous firing in ML, MR neurons and correlation between the weight changes for the
fake tasks... whether or not topography is needed for ML,MR is unclear - random inputs may allow more complex encodings.
but can at least try increasing populaion size of ML, MR.

try that out ...

also, looks like noise inputs are off ... may want to reintroduce them ...
other source of artificial synchronization is the image inputs, which arrive to the network at a fixed interval (~30 ms?)

adjusted some of the convergence/weights onto ML,MR to have same convergence from each V1,V4,IT source
and lower weights 1/2 of original to prevent too high firing ...

savefig('gif/20feb28_rast_a0.png')
savefig('gif/20feb28_reward_weights_a1.png')
some separation between them in all projections ...
savefig('gif/20feb28_action_freq_a2.png')

try same, but longer ...

125 s ...

  Cells: 1359
  Connections: 38281 (28.17 per cell)
  Spikes: 484922 (2.85 Hz)
  Simulated time: 125.0 s; 16 workers
  Run time: 2823.09 s
Saving output as data/20feb28_A0_simConfig.pkl ... 
Finished saving!
Done; saving time = 97.79 s.

savefig('gif/20feb28_rast_b0.png')
rates go up too much ... another indication that need some form of homeostasis

savefig('gif/20feb28_reward_weight_b1.png')

#
ldown,lup = [],[]
ltt = linspace(0,124,125)
for tt in ltt:
  actrewards = actreward[(actreward.action==3) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  ldown.append(len(actrewards))
  actrewards = actreward[(actreward.action==4) & (actreward.time>=tt*1e3) & (actreward.time<=(tt+1)*1e3)]
  lup.append(len(actrewards))

figure(); plot(ltt,ldown,'r',linewidth=4); plot(ltt,lup,'b',linewidth=4); xlabel('Time (s)'); ylabel('Move frequency (Hz)');
import matplotlib.patches as mpatches
lpatch = [mpatches.Patch(color=c,label=s) for c,s in zip(['b','r'],['Up','Down'])]
ax=gca()
ax.legend(handles=lpatch,handlelength=1)
xlim((0,125))

savefig('gif/20feb28_move_rate_b2.png')
ok, clearly moving up more than down after training ... (as it should)

can try training in DOWN direction as precaution or run actual game test ...

will try game test ...

ok running this with no fake rule, and duration of 250e3 ... 
"20feb28_G0_"

if there was a weaker opponent, might be easier to train as starting point?
or play against self ...
does openai allow controlling difficulty level of opponent?

well, did see the model score a point...but pretty rare so far ...
intermediate rewards after ball contacts racket might be sensible, though less generic...
can also have multidimensional error for reward/punishment, consisting of distance
between racket/ball (0.25), contact with racket (0.5), actual points (+1/-1)
or sign(current distance to ball - last distance to ball) -- but once using those
types of rules, no longer need the visual input at all ... 

separately, also need to be able to save/restore learned weights ...

stoped sim ~1/2 way through since it ends up mostly staying still, probably due to lot of punishments, as discussed before ...

openai allows easily saving mp4 videos ... added that option ...

ok, will set wbase to starting level, since starts with pretty low level of firing anyway ...

can also reduce the frequency of saving the synaptic weights ...

  Cells: 1359
  Connections: 38281 (28.17 per cell)
  Spikes: 986134 (1.45 Hz)
  Simulated time: 500.0 s; 24 workers
  Run time: 11678.73 s
  Saving output as data/20feb28_G0_simConfig.pkl ...

savefig('gif/20feb28_rast_g0.png')  

movie is useful: videos/20feb28_G0_/openaigym.video.0.24912.video000000.mp4
but only lasts a single episode  which took ~35 s, so lost viewing movie activity from most of the sim ...

simdat

savefig('gif/20feb28_reward_weights_g0.png')  

reward frequency might increase over time ... ? weights certainly have not stabilized yet
and although rewards compared to punishments are much less frequent, they still influence
the weights substantially - the weights are not at minimum ...

so, need to allow movies generated to encompass entired simulation, and to allow setting
the initial weights to the final ones generated by previous run ...

*20feb29
** run longer - check if video saved after each episode

ran but did not save data:
Done; run time = 37584.81 s; real-time ratio: 0.04.
ran out of memory...

Gathering data...
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
[zn:32842] *** Process received signal ***
[zn:32842] Signal: Aborted (6)
[zn:32842] Signal code:  (-6)
[zn:32842] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12890)[0x7f3ec802f890]
[zn:32842] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f3ec7c6ae97]
[zn:32842] [ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f3ec7c6c801]
[zn:32842] [ 3] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x8c957)[0x7f3ec84e0957]
[zn:32842] [ 4] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x92ab6)[0x7f3ec84e6ab6]
[zn:32842] [ 5] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x92af1)[0x7f3ec84e6af1]
[zn:32842] [ 6] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x92d24)[0x7f3ec84e6d24]
[zn:32842] [ 7] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x9329c)[0x7f3ec84e729c]
[zn:32842] [ 8] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(+0x1bd7a)[0x7f3ea9420d7a]
[zn:32842] [ 9] /usr/site/../arch/nrn/x86_64/lib/libnrniv.so.0(+0xb4227)[0x7f3eca326227]
[zn:32842] [10] /usr/site/../arch/nrn/x86_64/lib/libnrnoc.so.0(hoc_call_ob_proc+0x23a)[0x7f3eca5f1d7a]
[zn:32842] [11] /usr/site/../arch/nrn/x86_64/lib/libnrnoc.so.0(hoc_object_component+0x51e)[0x7f3eca5f2a8e]
[zn:32842] [12] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(+0xef3a)[0x7f3ea9413f3a]
[zn:32842] [13] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(+0x141fb)[0x7f3ea94191fb]
[zn:32842] [14] /usr/site/../arch/nrn/x86_64/lib/libnrniv.so.0(_ZN10OcJumpImpl7fpycallEPFPvS0_S0_ES0_S0_+0x3e)[0x7f3eca2ff5fe]
[zn:32842] [15] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(+0xeb37)[0x7f3ea9413b37]
[zn:32842] [16] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(_PyObject_FastCallDict+0x8a)[0x7f3ea97d424a]
[zn:32842] [17] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(+0x2084b7)[0x7f3ea98504b7]
[zn:32842] [18] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(_PyEval_EvalFrameDefault+0x3373)[0x7f3ea97a6153]
[zn:32842] [19] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(+0x208063)[0x7f3ea9850063]
[zn:32842] [20] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(+0x208616)[0x7f3ea9850616]
[zn:32842] [21] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(_PyEval_EvalFrameDefault+0x3373)[0x7f3ea97a6153]
[zn:32842] [22] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(+0x208063)[0x7f3ea9850063]
[zn:32842] [23] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(PyEval_EvalCodeEx+0x3e)[0x7f3ea9850a6e]
[zn:32842] [24] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(PyEval_EvalCode+0x1c)[0x7f3ea97a2b2c]
[zn:32842] [25] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(PyRun_FileExFlags+0xb7)[0x7f3ea97344d7]
[zn:32842] [26] /usr/site/nrniv/local/python/anaconda3/envs/py36/lib/libpython3.6m.so(PyRun_SimpleFileExFlags+0xf4)[0x7f3ea9738734]
[zn:32842] [27] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(nrnpy_pyrun+0x2c)[0x7f3ea941323c]
[zn:32842] [28] /usr/site/../arch/nrn/share/nrn/../../x86_64/lib/libnrnpython3.so(nrnpython_start+0x288)[0x7f3ea9413888]
[zn:32842] [29] nrniv(ivocmain+0x5cb)[0x563f75882bfb]
[zn:32842] *** End of error message ***
--------------------------------------------------------------------------
mpiexec noticed that process rank 0 with PID 0 on node zn exited on signal 6 (Aborted).
--------------------------------------------------------------------------

so will probably have to run simulation for shorter duration ...

*20mar1
** run 750 s again - adjust critic for punishments

can try critic with -0.1 or -0.01 so it's less pronounced compared to reward

750 s, with critic at -0.01 ...

myrun 24

*20mar2
** continue - look at output from last sim

  Cells: 1359
  Connections: 38281 (28.17 per cell)
  Spikes: 3478200 (3.41 Hz)
  Simulated time: 750.0 s; 24 workers
  Run time: 18652.00 s
Saving output as data/20mar1_G2_simConfig.pkl ... 
Finished saving!
  Done; saving time = 83.59 s.
Plotting recorded cell traces ... cell
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-samn'
Plotting raster...
Saving figure data as data/20mar1_G2_RasterData.pkl ... 
  Done; plotting time = 1282.06 s

savefig('gif/20mar2_G2_Raster.png')
  
savefig('gif/20mar2_G2_reward_weights.png')

ok, weights do not have as much time to decay now ...

but still need a way to save/restore the weights, and produce videos capturing whole simulation ...

*20mar5 - testing on laptop after conda/neurosim setup 
** conda/packages

had installed anaconda with python36 and then recompiled neuron, etc.

for ffmpeg if you're on ubuntu & have root can use sudo apt-get install ffmpeg

py3env <<-- that now sets conda to use py36
conda install numpy
conda install scipy
conda install pandas
conda install matplotlib
used new version of netpyne from github (development branch) with pip after activating the conda
env above; note that netpyne does not support latest matplotlib or latest pandas (sal mentioned
bug in those latest versions)

this is the alias in .tcshrc for setting the py env with conda:
alias py3env 'alias py3env setenv NSUF 'py3';source /usr/site/config/nrnenv.csh; setenv PYTHONPATH {$PYTHONPATH}:/usr/site/nrniv/local/python; conda activate py36'

once that env is activated can just use pip (it points to the right pip3 for that py env) and
don't have to explicitly indicate pip3

myrun 16

here's first error:

ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found
(required by
/usr/site/nrniv/local/python/anaconda3/envs/py36/lib/python3.6/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-36m-x86_64-linux-gnu.so)

according to : https://github.com/facebookresearch/detectron2/issues/609
should do:
conda update libgcc

PackageNotInstalledError: Package is not installed in prefix.
  prefix: /usr/site/nrniv/local/python/anaconda3/envs/py36
  package name: libgcc

conda install libgcc

ok, that fixed that problem ...

next error:
python
import sim
ModuleNotFoundError: No module named 'skimage'

conda install skimage

not found ...

it's scikit-image

conda install scikit-image

and also need openai gym
can install with conda as:
conda install -c akode gym

hmm, did not find it ...

ok, just use pip ...

pip install gym

pip install 'gym[atari]'

sudo add-apt-repository ppa:ubuntu-toolchain-r/test
sudo apt-get update
sudo apt-get install gcc-4.9
sudo apt-get install --only-upgrade libstdc++6

ok, that fixed error ... based on this:
 https://github.com/lhelontra/tensorflow-on-arm/issues/13#issuecomment-489296444

myrun 12

also have this in pythonpath:
 setenv PYTHONPATH {$PYTHONPATH}:$ND/share/python/lib/python:/usr/site/nrniv/local/python

*20mar6 - long run with restarts?
** testing

since haroon implemented weight save/restore, could let it run a few times with restarts ...

may not need to save video for now ... could instead run video once run a few rounds of training
...

*20mar8 - notes on changes
** started setting up multistepSim.py
which generates run file to call a few sims in a row, with each one reloading weights saved from
end of previous run; still testing - not working properly yet

** noticed on mar6 that the temperature was wrong (6.3 instead of 37)
changing temperature required readjusting the connection weights between cells
added the standard cfg. EEGain, EIGain, IEGain, IIGain to allow easier modulation of
these weights

this also required changing threshold for spikes - however, voltage traces do not currently
look great, so this will require some further adjustment. may want to use simple models of
PV cells nad or PYR cells ... although their dynamics with multiple ion channels will be
more costly than simpler standalone HH

** also changed population names so they'd be consistent
some populations seemd to have inconsistent names
now always using E as prefix for excitatory population and I as prefix for inhibitory population

** ran fake up rule test with new setup

produced higher weights for expected population

*20mar8 - fix for multistepsim, run test
** multistep test - still not working properly? fixed ... 

python multistepSim.py sim.json 16 2 multirun

individual sims do not stop after plot ... ?

ok, fixed problems ... conf.py was not reading correct json, was not passing it to sim.py either, etc.

** replace cell types?

voltage traces don't look great now after temp adjustment ...

** try a multistepsim test run with game (300 s run with 12 steps = 1 hr.)

not using fake rules ...

python multistepSim.py sim.json 16 12 multirun

*20mar9 - adjustments to cell types/architecture
** code reorg - moved mod files to mod subdir; moved cell types to cells subdir; compile script (nrnivmodl mod)
** HA added Mainen PYR2 and FS basket cells to model
this then required adjusting weights and...
** adjusted rules to have AMPA synapses of PYR on dend; AMPA of I cells on soma; GABAA for E,I on soma
** ...then had to adjust architecture slightly (new interneuron population)
to include IM interneuron population in motor area; this reduces likelihood of depolarization blockade
of the EMR and EML populations (seems to work after some adjustment to the weights)
a 10 s run shows one population of EM neurons firing faster than the other leading to paddle
staying at top of screen for much of the sim - not sure where this assymetry comes from

right now there's one interneuron population in M area providing feedback inhibition onto both EM and ER
populations; in future may want multiple interneuron populations that receive excitation from one population
and suppress the other (easy to implement in netpyne, but thought that might lead to further positive feedback
and one EMR vs EML population dominating the dynamics)

still, where does the assymetry come from?

*20mar10 - adjustments to connectivity, thresholds, E/I balance
** using dnumc as dictionary for number of cells
** set different thresholds for E and I cells (lower for I cells) via cellRule

netParams.cellParams['PYR_Mainen_rule']['secs']['soma']['threshold'] = 0.0
netParams.cellParams['FS_BasketCell_rule']['secs']['soma']['threshold'] = -10.0

** lack of symmetry fix? use convergence instead of probability

probably due to higher/lower number of inputs from IM to EMR vs EML
can fix that via using convergence rather than random connectivity

conv = pmat[from][to] * numc[from]

ok, try that with fake up rule ... 

myrun 16

hmm, something messed up ... was getting too much dep blockade ... 

ok, after adjusting the thresholds, other connection weights and gains ...

(note that IV4 was acting differently since receives extra inhibition from the large IV1 population; reduced
the weights from IV1 -> IV4 to reduce that effect)

get more reasonable raster, with higher I than E rates ... 

savefig('gif/20mar10_rast_a0.png')

testing fake up rule for 10 s ... still stable at end ...

raster looks ok:
savefig('gif/20mar10_rast_a1.png')
cells look ok:
savefig('gif/20mar10_IM_a1.png')
savefig('gif/20mar10_EMR_a1.png')
savefig('gif/20mar10_EML_a1.png')
savefig('gif/20mar10_IIT_a1.png')
savefig('gif/20mar10_EIT_a1.png')
savefig('gif/20mar10_IV4_a1.png')
savefig('gif/20mar10_EV4_a1.png')
savefig('gif/20mar10_IV1_a1.png')
savefig('gif/20mar10_EV1_a1.png')
savefig('gif/20mar10_IR_a1.png')
savefig('gif/20mar10_ER_a1.png')

and the weights ?
simdat
savefig('gif/20mar10_reward_weights_a1.png')
weights look biased towards correct population
(note that only recording weights every 1 s here, while reward signal recorded every 0.1 s so
cannot always see correspondence closely in figure above)

** Note: should change R and L to up and down (as appropriate)!
EMR, EML population names

** meanwhile, try multistep test again

python multistepSim.py sim.json 16 12 multirun

that did not seem to work ... paddle ends up becoming stationary after a while ...
probably too much punishment suppressing EMR,EML

*20mar11
** add noise?
** check haroon's updated intermediate reward rules

has these in sim.json:
"rewardcodes": {"scorePoint": 1, "losePoint": -0.01, "followBall": 0.001, "avoidBall": -0.001, "hitBall": 0.05},

will reward following ball and hitting ball more ... take a look to see if works, etc.

can try:
"rewardcodes": {"scorePoint": 1, "losePoint": -1, "followBall": 0.5, "avoidBall": -0.001, "hitBall": 0.75},

myrun 16

*20mar24
** discussion on retinal circuitry, movement selectivity

14:22
samn this paper looks useful https://senselab.med.yale.edu/ModelDB/ShowModel?model=116837&file=/RM_STDP/#tabs-1
14:23
https://paperpile.com/app/p/f2db9a70-4380-0c3c-9bc0-36a1fc7402f9
14:24
haroon found this paper for motion selectivity in retina: https://paperpile.com/app/p/7b54db25-10b2-0afe-af21-4538acf0def0
14:26
not sure if that model has retina with motion selectivity
14:28
here's another: https://senselab.med.yale.edu/ModelDB/ShowModel?model=118524#tabs-1
14:28
"virtual retina"
14:28
Haroon Anwar looks very phenomenological
14:29
samn https://paperpile.com/app/p/4c5b9cb8-6eb7-00f2-b3c9-fa42ea6d2595
14:30
"Abstract We propose a new retina simulation software, called Virtual Retina, which transforms a video into spike trains. Our goal is twofold: Allow large scale simulations (up to 100,000 neurons) in reasonable processing times and keep a strong biological plausibility, taking into account implementation constraints. The underlying model includes a linear model of filtering in the Outer Plexiform Layer, a shunting feedback at the level of bipolar cells accounting for rapid contrast gain control, and a spike generation process modeling ganglion cells. We prove the pertinence of our software by reproducing several experimental measurements from single ganglion cells such as cat X and Y cells. This software will be an evolutionary tool for neuroscientists that need realistic large-scale input spike trains in subsequent treatments, and for educational purposes"
14:30
maybe worth trying that if it has motion selectivity ... or check if can replicate relevant aspects of model
14:32
Haroon Anwar doesn’t look like- as i didnt find any keyword ‘direction’
14:33
samn does it emerge from the circuitry?
14:35
that one or different one, let's see if we can find good model that has the features needed and/or whether to replicate
14:38
Haroon Anwar may be--but if we can come up with a simpler circuit to implement direction selectivity of  ganglion cells, would prefer that….seems not difficult
14:38
samn ok - if you have an idea
14:38
Haroon Anwar the difficult part is how such information is carried out along the dorsal stream
14:39
and presented in V1, IT etc
14:39
samn if you have a simple circuit that can accomplish what we need, good
14:39
Haroon Anwar at input level. should be straight forward
14:39
at later stages-not sure
14:40
samn ok, can try first stages first...then wire it to higher after
14:40
Haroon Anwar ok
14:40
thanks
14:40
samn was that figure you shared from review paper enough to produce direction selectivity?
14:41
Haroon Anwar this shows the circuitry involved ----so its part of it…not complete
14:42
Screen Shot 2020-03-24 at 2.12.16 PM.png 
Screen Shot 2020-03-24 at 2.12.16 PM.png
14:42
how we arrange Bipolar inputs is not shown here
14:43
topologically bipolar inputs
14:43
samn some of this may be relevant: some of these papers look relevant too: https://www-sop.inria.fr/members/Pierre.Kornprobst/
14:43
hmm, since you already have the temporally decaying activation ... would it be simpler to take differences between current and previous frame and calculate direction vectors then feed to another population?
14:44
to avoid additional detailed circuitry
14:45
Haroon Anwar minimally we will have to add neurons which are not only showing position but all direction
14:45
right now its only position
14:45
samn right
14:45
Haroon Anwar so need more for direction
14:45
samn just question of whether to have circuit compute directions or to estimate it ourselves from images (optic flow)
14:46
Haroon Anwar yes thats simple way, can do that
14:46
samn ok sg

WikipediaWikipedia
Optical flow
Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene. Optical flow can also be defined as the distribution of apparent velocities of movement of brightness pattern in an image. The concept of optical flow was introduced by the American psychologist James J. Gibson in the 1940s to describe the visual stimulus provided to animals moving through the world. Gibson stressed the importance of optic flow for affordance perception, the ability to discern possibilities for action within the environment.  Followers of Gibson and his ecological approach to psychology have further demon… Show more(531 kB)
https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Opticfloweg.png/1200px-Opticfloweg.png
14:46
probably some of those image processing functions you're using can calculate that for you
14:46
Haroon Anwar right
14:47
OK - so will include the direction precomputed
:+1:
1
14:47
samn sg
14:50
if anyone else saw relevant approaches, let us know

*20mar25
** zlib error

cd ~/SM*
py3env
myrun 16

ImportError: /lib/x86_64-linux-gnu/libz.so.1: version `ZLIB_1.2.9' not found (required by
/usr/site/nrniv/local/python/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/../../.././libpng16.so.16)

https://stackoverflow.com/questions/48306849/lib-x86-64-linux-gnu-libz-so-1-version-zlib-1-2-9-not-found

fix?

download:
https://sourceforge.net/projects/libpng/files/zlib/1.2.9/zlib-1.2.9.tar.gz/download

then:
cd ~/Downloads
tar -xvf ~/Downloads/zlib-1.2.9.tar.gz
cd zlib-1.2.9
sudo -s
./configure; make; make install
cd /lib/x86_64-linux-gnu
ln -s -f /usr/local/lib/libz.so.1.2.9/lib libz.so.1
cd ~/Downloads
rm -rf zlib-1.2.9

cd ~/SM*
myrun 16

ok, runs now without an error ...

** check dynamics
** how to implement direction selectivity

use optical flow on successive images to produce movement vectors at each coordinate.

then, need at least 4X number of pixels to represent the movement info from frame to frame?
and it has to be topographically arranged

...where does each pixel of movement selective neurons project?

*20mar31
** back to testing

myrun 16

*20apr1
** fixing up plotSpatioTemporalActivity.py

myrun 16

python -i plotSpatioTemporalActivity.py

seems to work now ... there weren't any loops, functions, dictionaries, etc.
really used in the file ...

*20apr7
** sal code for animation

def animateRateVsWeight(dataFolder, batchLabel, params):
    import imageio
    from pathlib import Path
    Lvals = params[0]['values']
    Ivals = params[1]['values']
    for ipop, pop in enumerate(Lvals):
        print('Generating traces gif for pop %s ...' % (pop))
        #v22_batch1_18_98_traces.png
        images = ['%s/%s/%s_%d_%d_traces.png' % (dataFolder, batchLabel, batchLabel, ipop, iweight) for iweight in range(len(Ivals))]
        #images = list(image_path.glob())
        image_list = []
        for file_name in images:
            image_list.append(imageio.imread(file_name))
        pass
        imageio.mimwrite('%s/%s/%s_%s_traces.gif' % (dataFolder, batchLabel, batchLabel, pop), image_list)

should adapt for plotSpatioTemporalActivity.py to allow saving output
and/or for video ...

** test update

a lot more cells now (1e3 more motor cells, and 800 direction selective cells), so runs slower,
+ image processing slows it down some more ... not clear by how much

python -i plotSpatioTemporalActivity.py

fig, axs, plt = plotActivityMaps(pauset=0,gifpath='20apr7_activity.gif')

*20apr8
** save spatiotemporal activity as movie instead (using ffmpeg)

could use imageio ffmpeg interface
https://imageio.readthedocs.io/en/stable/format_ffmpeg.html#parameters-for-saving

or ffmpeg-python https://github.com/kkroening/ffmpeg-python
though that is more comprehensive, so may not need full package installed ...

https://imageio.readthedocs.io/en/stable/examples.html#writing-videos-with-ffmpeg-and-vaapi

https://github.com/imageio/imageio-ffmpeg

pip install imageio-ffmpeg

should have used conda to install
with
conda install imageio-ffmpeg -c conda-forge
but without the ffmpeg binary

python -i plotSpatioTemporalActivity.py

lfnimage = ['/tmp/'+str(x)+'.png' for x in range(1,50,1)]
limage = [imageio.imread(fn) for fn in lfnimage]


from imageio_ffmpeg import write_frames

w = imageio.get_writer('my_video.mp4', format='FFMPEG', mode='I', fps=1,
                       codec='h264_vaapi',
                       output_params=['format=gray'],
                       pixelformat='gray')

for img in limage: w.append_data(img)                       

w.close()

gen = write_frames('test.mp4', (limage[0].shape[0],limage[0].shape[1],limage[0].shape[2]), pix_fmt_in="gray")
gen.send(None)  # seed the generator
for img in limage: gen.send(img)
gen.close()  # don't forget this

hmm, getting errors ... ffmpeg-python seems easier to use?

pip install ffmpeg-python

import ffmpeg
(
    ffmpeg
    .input('/tmp/*.png', pattern_type='glob', framerate=10)
    .output('movie.mp4')
    .run()
)

that works ... fast to run and makes small files

ok, put that into plotSpatioTemporalActivity.py ...

and added animation saving functions to anim.py

** testing network

number of IM was too low, after the increase in EMR, EML populations
so increased IM to 690, to be about 20% of IM (690) + EMR (1350) + EML (1350)

now, when run network, get too high firing rates for EMR, EML

savefig('gif/20apr8_rast_a0.png')

is same obtained with fewer IM cells?

reset IM to 50 ... 

myrun 12

hmm, now EMR,EML firing rates look better, but not clear why. no depolarization
blockade seen in EMR,EML

savefig('gif/20apr8_rast_a1.png')

check connectivity in sim.py ... something off?

might be due to using fixed convergence instead of probability ... for larger pop sizes
seems better to use probability ... had used convergence before to ensure minimum number
of inputs (more relevant with smaller populations)

try with probability of 0.25 and larger IM population of 690 ...

myrun 12

ok, rates of EMR, EML went down, and IM rates are OK
but now activity looks highly synchronized to 100 ms interval when visual inputs come in:
 gif/20apr8_rast_a2.png
 gif/20apr8_EMR_a2.png
 gif/20apr8_IM_a2.png

so, should avoid it ... probably can reduce weights between EMR->IM, EML->IM, IM->IM, IM->EMR, IM->EML

can try cutting probabilities between those populations in half (from 0.25 to 0.125)

myrun 12

savefig('gif/20apr8_rast_a3.png') 
looks ~same

try cutting down probabilities further ... 0.0625

myrun 12

little better ...

gif/20apr8_rast_a4.png
gif/20apr8_EMR_a4.png
gif/20apr8_IM_a4.png

should run longer to see if remains stable ... 
some of the other E vs I populations have wrong rates, e.g. EV4 faster than IV4, EV1 faster than IV1
EMT only a little slower than IMT
and no inhibitory populations in the direction selective cells...

run for 5 s to see if patterns similar, then may consider adjusting weights/probabilities further ...

myrun 12

ok, looks like decent rates ... less synchrony ... should be OK for now ... can adjust further as needed ... 
gif/20apr8_rast_a5.png
gif/20apr8_rast_a5b.png <- some alternation between synch and asynch state
gif/20apr8_IM_a5.png
gif/20apr8_EMR_a5.png
gif/20apr8_EML_a5.png

python -i actmap.py

made this movie: data/20april08_A0__movie.mp4

** try longer sim ("name":"20april08_B0_")

to test and watch output ...

"rewardcodes": {"scorePoint": 1, "losePoint": -0.01, "followBall": 0.01, "avoidBall": -0.001, "hitBall": 0.25},

and duration of 100 s ...

myrun 12

started ~17:32 ...

finished @ ~22:15 ...

rates ok:
 gif/20apr8_rast_b0.png
 gif/20apr8_rast_b0b.png

cells look ok?
 gif/20apr8_IM_b0.png
 gif/20apr8_EML_b0.png
 gif/20apr8_EV1DSE_b0.png
 gif/20apr8_EV1DS_b0.png
 gif/20apr8_EV1_b0.png
 yeah, most cells look ok. some populations fire much less than others.
 direction cells - many directions not firing too mcuh (E, W), could be due to the
 ball mostly moving at a diagonal; but N,S higher since paddles move up,down
 
and now to create the movie ...

python -i actmap.py

*20apr9 - trying to speed up animation production
** continue

hmm, took > 12 hours so far to spit out the pngs up to 66 s, and hasn't even started on movie creation yet ...

should stop it and figure out how to speed up that process ...

can make movie from the frames that were produced so far

python
import anim
anim.savemp4('/tmp/*.png', 'data/20april08_B0_actmap.mp4', 10)

very slow encoding of mp4 ...

there's support in matplotlib for making animations and exporting to mp4 via ffmpeg but that's
slow too ...

imagemagick gif writing faster?

sudo apt-get install imagemagick

already have imagemagick ...

hmm, even the imagemagick gif writer is slow . . .

*20apr13 - animation fixing
** HA fixing collision detection code since not working in all cases
algorithm relies on position, direction, score
** movie fix

would concat of smaller mp4 files together run faster than producing one giant mp4?

https://stackoverflow.com/questions/7333232/how-to-concatenate-two-mp4-files-using-ffmpeg

could try ffmpeg concat demuxer : 

"Use this method when you want to avoid a re-encode and your format does not support file level
concatenation (most files used by general users do not support file level concatenation).

$ cat mylist.txt
file '/path/to/file1'
file '/path/to/file2'
file '/path/to/file3'

$ ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4"

hmm, problem seems to be that matplotlib takes longer and longer to save output files
in beginning, saves 15 files per minute, then gradually decreases to 5 per minute ...

and that's particularly true when setting figsize to high resolution values ...

might be due to matplotlib slowing down with all the redrawing ... not even having to do
with file saving ...

yes, not ffmpeg issue - ffmpeg runs very quickly once all frames are available ... so that
isolates slowness to matplotlib

this related issue mentions slowing down:
https://github.com/matplotlib/matplotlib/issues/16182

https://stackoverflow.com/questions/40747181/slow-ploting-using-animation-function-in-matplotlib-python
also relevant ...

fig, axs, plt = animActivityMaps(mp4path='test.mp4', framerate=10)

*20apr14
** HA made nice movie showing activity/dynamics/actions from random game (testPong.py)
will use it to generate similar with additional inclusion of network dynamics

** other movie fixes needed for simdat.py

first rerun sim for 10 s ... since now have a new column in ActionsRewards.txt

myrun 12

  Cells: 5349
  Connections: 1103176 (206.24 per cell)
  Synaptic contacts: 1105826 (206.74 per cell)
  Spikes: 78992 (1.48 Hz)
  Simulated time: 10.0 s; 12 workers
  Run time: 1974.00 s
Saving output as data/20april14_A0_simConfig.pkl ... 
Finished saving!
  Done; saving time = 82.23 s.
SAVING RASTER DATA
plot raster:
Plotting raster...
QApplication: invalid style override passed, ignoring it.
Saving figure data as 20april14_A0_raster.pkl ... 
Plotting recorded cell traces ... cell
Plotting raster...
Saving figure data as data/20april14_A0_RasterData.pkl ... 
  Done; plotting time = 30.80 s

Total time = 2191.62 s

End time:  2020-04-14 12:31:15.408575

output looks ok ... cells and rates in raster

20apr14_rast_a0.png
20apr14_rast_a0b.png
20apr14_EV1DN_a0.png
20apr14_EV1DS_a0.png
20apr14_EV1DSW_a0.png
20apr14_IMT_a0.png
20apr14_IV4_a0.png

let's see actmap.py ... then simdat.py

python -i actmap.py

produces
data/20april14_A0_actmap.mp4
fairly quickly...

and simdat.py ...

python -i simdat.py -1

that calls
plotSynWeightsPerTimeStep(pdf,pauset=1,mp4path='data/'+dconf['sim']['name']+'weightmap.mp4', framerate=10) #plot images

python -i simdat.py -1

** discussion on smooth direction selective RFs

13:17
samn btw, should E neurons always fire (but at a slower rate) when NE neurons fire?
13:17
since they're not orthogonal directions
13:17
Haroon Anwar no
13:17
samn why
13:18
Haroon Anwar E is between 337.5 degrees and 22.5 degrees
13:18
and NE is between 22.5 and 67.5 degrees
13:19
360 is divided into 8 angular regions
13:19
and each population is assigned that
13:19
samn understand but that means sharp cutoffs
13:19
Haroon Anwar right
13:19
samn could also have smooth fall-off and smoother receptive fields
13:19
Haroon Anwar possible
13:20
might be a good idea…
13:20
samn can put that on list for later
13:20
seems more realistic (?)
13:21
Haroon Anwar sure…more realistic
13:21
samn do all neurons of a population have exact same receptive field?
13:21
Haroon Anwar will also reduce number of neurons
13:21
because we could have directions coded using 4 pops instead of 8
13:22
NE will evoke e.g. 5 Hz in N and 5 Hz in E
13:22
if its exactly 45 degrees
13:22
samn you could have it with 1 pop probably too if you had receptive fields with some width tuning
13:22
width to the receptive field around a mean angle
13:22
Haroon Anwar that would be unrealistic
13:23
samn why?
13:23
Haroon Anwar we have neurons with very fine tuned directions
13:23
1 pop would not be able to encode all directions
13:23
samn it would be 1 pop in the model
13:23
where each neuron selects a mean angle randomly
13:23
then overall you'd have representations in all directions
13:24
Haroon Anwar ok --- yes possible that way---
13:24
implementation might be a bit tricky----but yes possible
13:25
samn tricky for how they project to other areas?
13:25
Haroon Anwar or assigning firing rates to each of the neurons in that pop
13:25
and generating connection lists
13:25
but yeah thats more realistic
13:26
samn if each one had a location in space and there were enough of the full directions in any locatin, seems ok
13:26
anyway, can put that on list of things to adjust. reducing # of neurons could help as you said
13:27
Haroon Anwar yes definitely to do list--- i need to think.. may be its not as difficult as i think right now…. but i will think about it
13:27
samn ok sg
13:28
Haroon Anwar will look at it after performance analysis---
13:28
samn sg

*20apr15
** continue fixup of animSynWeights in simdat.py

this example animation https://matplotlib.org/gallery/animation/basic_example.html
shows how to set line data dynamically ...

and simpler way to save to mp4

animSynWeights(pdf,'data/'+dconf['sim']['name']+'weightmap.mp4', framerate=10) #plot/save images as movie

ok, works faster now ...

but some of the text is not visible ... adjust size of fig

python -i simdat.py -1
animSynWeights(pdf,'data/'+dconf['sim']['name']+'weightmap.mp4', framerate=10, figsize=(8,6)) #plot/save images as movie

animSynWeights(pdf,'data/'+dconf['sim']['name']+'weightmap.mp4', framerate=10, figsize=(7,5)) #plot/save images as movie

also turn off interactive mode during animation, goes faster...

animSynWeights(pdf,'data/'+dconf['sim']['name']+'weightmap.mp4', framerate=10, figsize=(7,4)) #plot/save images as movie

** discuss opt (image processing for direction selective neurons vs population reduction)
** looking through code for what to optimize/improve

moved some connection functions from sim.py to connUtils.py

cleaned up some of the code that calculates firing rates based on image contents
to use dictionaries and loops instead of previous code duplication ...

that cleanup will help with modifications in future ...

tested network with short run after first adjustments

myrun 12

20april15_A0_rast.png
20april15_A0_EMR.png
20april15_A0_IM.png
20april15_A0_EV4.png
20april15_A0_EV1DS.png

looks ok ...

check videos ...

python -i simdat.py -1

data/20april15_A0_weightmap.mp4

python -i actmap.py

data/20april15_A0_actmap.mp4

looks ok ... should try more thorough tests to make sure nothing broken ...

